# Brief History of Diffusion-based Generative models and their variants

* **BigGAN**: 2019. 2 (ICLR 2019) | DeepMind
* **Generative Modeling by Estimating Gradients of the Data Distribution**: 2019. 7 | Stanford
* **DF-GAN**: 2020. 8
    - Text-to-Image
* ğŸ“Œ **DDPM**: 2020. 12 (NeurIPS 2020) | Berkeley â†’ Google Research
* **DDIM**: 2020. 10 | Stanford
    - ê¸°ì¡´ DDPMì´ Noising processê°€ Markovianì´ë¼ ë°”ë¡œ ì´ì „ì˜ timestep noised imageì—ë§Œ ì˜ì¡´í•˜ì—¬ ì „ì²´ í”„ë¡œì„¸ìŠ¤ë¥¼ ë§Œë“¤ì–´ë‚´ëŠ”ë° ì‹œê°„ì´ ë§ì´ ê±¸ë¦°ë‹¤. ì—¬ê¸°ì„œ Non-markovianìœ¼ë¡œ $x_0$ì—ì„œ $x_T$ë¡œ ê°€ëŠ” ê³¼ì •ì„ deterministicí•˜ê²Œ forward processë¥¼ ì¼ë¶€ë§Œ samplingí•˜ì—¬ trainí•˜ê³  ì¶”ë¡ í•  ë•Œë„ ë§ˆì°¬ê°€ì§€ë¡œ ì „ì²´ stepì„ ë‹¤ ê±°ì¹˜ì§€ ì•Šì•„ accelerated inferenceê°€ ê°€ëŠ¥í•˜ë‹¤.
* **Score-based Generative Modeling through Stochastic Differential Equations**: 2020. 11 | Google Research
* **VQGAN**: 2020. 12 (CVPR 2021) | CompVis
    - ConvNetê³¼ Transformerê°€ generation taskì—ì„œ ê°ê° ì˜í•˜ëŠ” ë¶€ë¶„ì„ ë‹¤ ì‚¬ìš©í•´ì„œ image tokenizationì„ ì§„í–‰
        - ConvNetì€ inductive biasë¡œ ì¸í•´ local ì •ë³´ë¥¼ ë§ì´ ê°€ì§€ê³  ìˆë‹¤. í•˜ì§€ë§Œ global ì •ë³´ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šë‹¤.
        - Transformer modelì€ low inductive bias ë•ë¶„ì— expressiveí•˜ë‹¤ (ì›Œë”© ë•Œë¬¸ì— í˜¼ë€ìŠ¤ëŸ¬ìš´ë° ì•„ë§ˆ global informationì„ ë³´ìœ í•œë‹¤ ì •ë„ë¡œ ì´í•´í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œ ì‹¶ë‹¤). í•˜ì§€ë§Œ ê³ í™”ì§ˆì˜ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ê¸°ëŠ” ì–´ë µë‹¤.
    - ConvNetìœ¼ë¡œ image feature mapì„ ë§Œë“  í›„ ì´ë¥¼ quantizeí•˜ì—¬ latent image codeë¥¼ ë§Œë“ ë‹¤. ë˜í•œ ì´ codeë¥¼ decoderì— íƒœì›Œ GAN êµ¬ì¡°ë¡œ í•™ìŠµì„ ì‹œí‚¨ë‹¤.
    - ê·¸ í›„ imageë¥¼ tokenizeí•˜ì—¬ sequenceë¡œ ë³´ìœ í•œ í›„ ì´ë¥¼ language modelì²˜ëŸ¼ í•™ìŠµì‹œí‚¨ë‹¤ (Auto-regressiveí•˜ê²Œ next token prediction ê°€ê¹Œì›€)
    - Auto-regressiveí•œ íŠ¹ì„±ì„ í™œìš©í•˜ì—¬ conditioningì„ í•  ìˆ˜ ìˆë‹¤: ì‹œì‘ì ì— conditioning informationì„ prependí•´ì„œ ì¶”ë¡ 

* **XMC-GAN**: 2021. 1 (CVPR 2021) | Google Research
    - Text-to-Image


* ğŸ“Œ **Improved DDPM**: 2021. 2 | OpenAI
    - DDPMì˜ ì—¬ëŸ¬ ì„±ì§ˆë“¤ì„ ì´í•´í•˜ëŠ”ë° ì¤‘ìš”í•œ ë…¼ë¬¸. ê°œì¸ì ìœ¼ë¡œ ì–´ë–¤ ê±¸ íƒêµ¬í•˜ê¸° ìœ„í•´ ì´ê²Œ ì™œ ì˜ë˜ì—ˆëŠ”ì§€ ì‹¤í—˜í•´ë³´ë ¤ë©´ ì´ë ‡ê²Œ í•´ì•¼í•˜ì§€ ì•Šë‚˜ë¼ëŠ” ìƒê°.
        - DDPMì´ log-likelihood ë©´ì—ì„œ ë‹¤ë¥¸ Generative model ë³´ë‹¤ ë°€ë¦¬ëŠ” ì´ìœ 
            - log-likelihood ì‹¤í—˜í•˜ëŠ” ì¤‘ conditional-GANê³¼ ë¹„êµí•´ì•¼í•´ì„œ timestep embeddingì— class-embeddingì„ ê°™ì´ ë…¹ì—¬ë„£ì—ˆëŠ”ë° ì´ê²ƒì´ conditional-diffusionì˜ ì‹œì‘ì´ ì•„ë‹Œê°€ ë¼ëŠ” ìƒê°ë„ ë“ ë‹¤
        - Noise schedulingì„ linearì—ì„œ cosineìœ¼ë¡œ ë°”ê¾¸ì!: linearëŠ” í›„ë°˜ stepì—ì„œ ë„ˆë¬´ ì´ë¯¸ì§€ë‘ ê´€ë ¨ ì—†ëŠ” noise-to-noise recon ë°–ì— í•˜ì§€ ì•ŠëŠ”ë‹¤.
            - VLBë¥¼ prior / posteriorì˜ interpolateëœ ê²ƒì„ L_simpleì—ì„œ ê°™ì´ ì“°ë©´ ì˜ë‚˜ì˜¨ë‹¤ (í•˜ì§€ë§Œ ì´ë˜í•œ ë³´ì •ì´ í•„ìš”í–ˆìŒ)
            - ì´ ê³¼ì •ì—ì„œ interpolationì´ rescalingìœ¼ë¡œ ì¸í•´ shorter diffusionì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤... ì¦‰ ë” ì ì€ sampling stepì„ ì“¸ ìˆ˜ ìˆë‹¤ëŠ”ë° ì •í™•íˆ ì´í•´ëŠ” ê°€ì§€ ì•ŠëŠ”ë‹¤.
        - Sampling step Tê°€ í´ìˆ˜ë¡ ì˜ë‚˜ì˜¨ë‹¤(ì‚¬ì‹¤ ì–´ëŠì •ë„ ì˜ˆìƒí–ˆìŒ). ê·¸ëŸ°ë° Tê°€ ëŠ˜ë©´ samplingì´ ì•„ì£¼ ëŠë¦¬ë‹¤ â†’ ë¹¨ë¦¬ ë½‘ëŠ” ë²•ì„ ë– ì˜¬ë ¤ë³´ì•˜ë‹¤.
        - Model scaling ê´€ì ì—ì„œë„ ê³ ë¯¼ì„ í–ˆìŒ: ì•ìœ¼ë¡œ GPUëŠ” ë¹µë¹µí•´ì§ˆ ê²ƒì´ë¯€ë¡œ ë” í° ëª¨ë¸ë„ í…ŒìŠ¤íŠ¸í•´ë³´ì•˜ë‹¤...ã…‹ã…‹;
    
* **DallE**: 2021. 2 | OpenAI
    - Discrete variational autoencoder (DVAE)ë¡œ ì´ë¯¸ì§€ë¥¼ $32^2$ gridë¡œ ë§Œë“¤ì–´ì¤Œ. ì´ ë•Œ codebookì€ ì´ 8192ê°œì˜ ê³ ìœ  tokenì´ ìˆìŒ. ì´ë¥¼ 256 BPE-encoded text tokenìœ¼ë¡œ text queryë¥¼ toeknizeí•˜ê³  ë°©ê¸ˆ ë§Œë“  $32^2$ ì´ë¯¸ì§€ì˜ image tokenë“¤ê³¼ concatenate, autregressiveí•˜ê²Œ í•™ìŠµ
    - ì–˜ë„ Likelihood $\ln p_{\theta,\psi}(x, y)$ë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒìœ¼ë¡œ Lower boundë¥¼ í•™ìŠµí•œë‹¤.
* ğŸ“Œ **CLIP**: 2021. 3 | OpenAI
    - Generative modelingì´ë¼ê¸° ë³´ë‹¤ëŠ” Image-text pair í•™ìŠµ ë°©ë²•ë¡ ìœ¼ë¡œ ì´ë ‡ê²Œ í•™ìŠµëœ encoderë“¤ì„ í™œìš©í•œë‹¤.
* **SR3**: 2021. 4 | Google Research
* ğŸ“Œ **Guided Diffusion (Classifier Guidance, ADM: Ablated Diffusion Model)** 2021. 5 | OpenAI
    - Pre-trained classifierì˜ gradientë¥¼ conditionìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²• ì œì‹œ
    - ê·¸ ë°–ì—ë„ ëª¨ë¸ êµ¬ì¡° ê°œì„ ì— ëŒ€í•œ ê³ ì°° ì§„í–‰
        - Increased depth: ì„±ëŠ¥ì€ ë†’ì´ì§€ë§Œ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¼ â†’ discard
    - ì‚¬ì‹¤ GANì—ì„œ Classifier guidanceì™€ ë¹„ìŠ·í•œ íš¨ê³¼ë¥¼ ì£¼ëŠ” ê²ƒì€ truncation, temperature scalingì´ë‹¤. ê°™ì€ ê±¸ diffusionì—ë„ ì ìš©í•´ë³´ì•˜ìœ¼ë‚˜ ì˜ë˜ì§€ ì•Šì•˜ë‹¤: blurry/smooth images, low precision & recall
        - samplingí•  ë•Œ temperatureë§Œí¼ meanì€ scaling, deviationì€ ë‚˜ëˆ ì£¼ê¸°

* **Cascaded Diffusion (CDM)** 2021. 6 | Google Research
    - High-resolution fidelityë¥¼ ìœ„í•´ SRì„ ë‹¨ê³„ë³„ë¡œ ì§„í–‰
    - 
* **ImageBART**: 2021. 8 (NIPS 2021) | CompVis
* **Vit-VQGAN**: 2021. 9 | Google Research
* **LSGM**: 2021. 10
* ğŸ“Œ **Classifier Free Guidance**: 2021 NeurIPS workshop | Google Research
    - Classifier guidanceëŠ” Sampling ì¡°ì ˆì— ì—„ì²­ë‚œ ì˜í–¥ì„ ë¼ì³¤ìœ¼ë‚˜ Pre-trained classifierì˜ gradientë¥¼ ìš”êµ¬í•˜ê¸° ë•Œë¬¸ì— êµ‰ì¥íˆ computational burdenì´ ë†’ë‹¤.
    - ê·¸ í•˜ë‚˜ì˜ ëª¨ë¸ì´ conditionê¹Œì§€ ê°™ì´ ë°›ì•„ì„œ guidanceë¥¼ ìƒì„±í•˜ëŠ”ë°, ì´ ë•Œ ì˜ˆì¸¡í•˜ë ¤ëŠ” noise image = conditional + guidance_scale * (conditional - unconditional)
        - ì´ëŠ” p(c|z_t)ê°€ Bayes ruleì— ì˜í•´ p(z_t | c) / p(z_t)ì— ë¹„ë¡€í•˜ë‹¤ëŠ” ì ì„ logë¥¼ ì”Œì›Œ ìœ ë„í•œ ê²ƒìœ¼ë¡œ ì´í•´í•˜ë©´ ëœë‹¤. ì¦‰ ì´ ì´ë¯¸ì§€ë¥¼ í•™ìŠµì‹œí‚¨ë‹¤ëŠ” ê²ƒì€ p(c|z_t)ê°€ ê°€ëŠ¥í•˜ë„ë¡ ìƒì„±í•˜ëŠ”ê±´ë° t-stepì˜ noiseê°€ ë‚€ latent vectorë¡œë¶€í„° classë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ëœ»í•œë‹¤.
    - ? ì „ë¶€í„° ê¶ê¸ˆí–ˆëŠ”ë° ì—¬ê¸°ì„œ ì´ ë–„ê¹Œì§€ì˜ guidanceëŠ” text promptê°€ ì•„ë‹ˆë¼ ì¼ë°˜ì ì¸ ImageNet Class 1000ê°œë¥¼ ì„ë² ë”©í•´ì„œ ë„£ì–´ë†“ëŠ” í˜•ì‹ì¸ê°€? ì´ë ‡ê¸° ë•Œë¬¸ì— ë°”ë¡œ ë‹¤ìŒì— ë‚˜ì˜¤ëŠ” GLIDEê°€ text-promptë¥¼ CLIPìœ¼ë¡œ ë§Œë“¤ì—ˆë‹¤ëŠ” ê²ƒì— ëŒ€í•´ ìš”ì§€ë¥¼ ë‘ëŠ” ê²ƒ ê°™ìŒ

---

* **GLIDE**: 2021. 12 | OpenAI
    - Diffusion + Text Prompt Guidanceì˜ ì‹œì´ˆë¼ê³  ë³¼ ìˆ˜ ìˆìŒ (text-to-image synthesisë¥¼ ì²˜ìŒ í•œ ê²Œ ì•„ë‹ˆë¼ diffusionì—)
        - ì´ ë•Œ text encoder embeddingì„ CLIP Guidance vs. Classifier-free guidance ë‘ ê°€ì§€ ë‹¤ë¥¸ ë°©ë²•ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì—ê²Œ ì œê³µí•´ ì´ë¯¸ì§€ë¥¼ ìƒì„±í–ˆë‹¤. ë‘ ê°€ì§€ ë°©ë²•ìœ¼ë¡œ ë‚˜ì˜¨ ì´ë¯¸ì§€ë¥¼ ì‚¬ëŒì—ê²Œ ë³´ì—¬ì¤˜ì„œ(+automated evaluation) ëˆ„ê°€ ë” ì§„ì§œ ê°™ì€ì§€ ë¹„êµí–ˆì„ ë•Œ CFGê°€ ì´ê²¼ë‹¤. ì°¸ê³ ë¡œ ì—¬ê¸°ì„œ CLIP GuidanceëŠ” classifier-guidanceì¸ë°, classifierê°€ CLIPI text-encoderì¸ ê²½ìš°ë¥¼ ëœ»í•œë‹¤.
        - CFGê°€ photorealistic + wide breadth of world knowledgeë¥¼ ê°€ì§„ë‹¤ê³  íŒë‹¨. DallEë‘ ë¹„êµí–ˆì„ ë•Œ GLIDEê°€ ë” photorealisticí•˜ë‹¤ê³  85%ê°€ ë‹µë³€, captionê³¼ alignë˜ì—ˆë‹¤ê³  69%ê°€ ë‹µë³€
    - ì¼ë°˜ì ì¸ Diffusion modelì„ Text-promptë¥¼ ê°™ì´ ë„£ì–´ì„œ í•™ìŠµí•œ ë°©ì‹

* **MaskGIT**: 2022. 2 | Google Research
    - Auto-regressive ê¸°ë°˜ì˜ ì´ë¯¸ì§€ìƒì„± ëª¨ë¸ì€ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦°ë‹¤. ê·¸ë¦¬ê³  AR ë°©ì‹ ìì²´ê°€ ì‚¬ëŒì´ ê·¸ë¦¼ ê·¸ë¦´ ë•Œì˜ ìƒí™©ê³¼ ë¹„êµí•´ë³´ë©´ ë§ì´ ë˜ì§€ ì•ŠëŠ”ë‹¤ (ì‚¬ëŒì€ ê·¸ë¦¼ê·¸ë¦´ë•Œ ì™¼ìª½ ìœ„ë¶€í„° ì˜¤ë¥¸ìª½ ì•„ë˜ë¥¼ ì°¨ë¡€ë¡œ ì±„ìš°ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì—¬ê¸° ê·¸ë ¸ë‹¤ê°€ ì €ê¸° ê·¸ë ¸ë‹¤ê°€ í•˜ê¸° ë•Œë¬¸)
    - ì´ ë‘ ê°€ì§€ motivationìœ¼ë¡œë¶€í„° Parallel decodingí•œ ë°©ë²•ì„ ì œì‹œí•œë‹¤. ê¸°ì¡´ VQGANì²˜ëŸ¼ quantized image token -> reconí•˜ëŠ” taskì™€ ë™ì‹œì— MLMì˜ ë°©ì‹ê³¼ ë¹„ìŠ·í•œ MVTM(Masked Visual Token Modeling)ë¥¼ ìˆ˜í–‰í•˜ì—¬ decoding stage ë˜í•œ í•™ìŠµí•œë‹¤. ì´ëŠ” bidirectionalí•˜ê²Œ tokenì´ ë‹¤ë¥¸ ëª¨ë“  tokenë“¤ì„ ì°¸ì¡°í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹¤. (AR ë°©ì‹ì˜ ê²½ìš° ì•ì—ì„œë¶€í„° ì°¨ê·¼ì°¨ê·¼ decodeí•˜ë‹¤ë³´ë‹ˆ ë’¤ë¥¼ ì°¸ì¡°í•˜ì§€ëª»í•¨). ì¦‰ ì´ë¯¸ì§€ê°€ ë™ì‹œì— maskingê³¼ class token ì„œë¡œë¥¼ ì°¸ì¡°í•˜ë©° í˜¼ëˆì˜ ì¹´ì˜¤ìŠ¤ë¡œë¶€í„° ë§Œë“¤ì–´ì§€ëŠ” ì´ë¯¸ì§€ë„ê¹Œ. ê¸°ì¡´ì˜ 32*32=1024 í† í°ë§µì„ ì˜ˆì¸¡í•˜ëŠ” taskë¼ë©´ ë§ê·¸ëŒ€ë¡œ 1,024 stepì„ ë°Ÿì•„ì•¼ ì´ë¯¸ì§€ìƒì„±ì´ ê°€ëŠ¥í–ˆë‹¤ë©´, MaskGITì€ 8 ~ 12step ì•ˆì— ìƒì„±ì„ ë§ˆì¹  ìˆ˜ ìˆë‹¤.
    - ì ‘ê·¼ë„ ì‹ ì„ í–ˆê³  ablationë„ ì˜ëœí¸ì´ë¼ ì¢‹ì€ ë…¼ë¬¸ì´ë¼ê³  ìƒê°í•œë‹¤.

* ğŸ“Œ **Latent Diffusion (LDM, or a.k.a. Stable Diffusion)** 2022.4 | CompVis
    - Pixel spaceì—ì„œ ë°”ë¡œ ì˜ˆì¸¡í•˜ë©´ ì‹œê°„ì´ ë„ˆë¬´ ì˜¤ë˜ ê±¸ë¦¬ê¸° ë•Œë¬¸ì— latent spaceì—ì„œ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ ì—°êµ¬. í™•ì‹¤íˆ ì‹œê°„ì ì¸ ì¸¡ë©´ì—ì„œ ë‹¨ì¶•ì´ ë§ì´ ì´ë¤„ì¡ŒìŒ.
    - Latent representationì— DMì„ ê±°ëŠ” ê²ƒì— ëŒ€í•œ ê³ ì°°ì„ í–ˆìŒ
        - ê¸°ë³¸ì ìœ¼ë¡œ Generative modelingì€ perceptual / semanticí•œ ìš”ì†Œë¥¼ ì°¨ë¡€ë¡œ í•™ìŠµí•˜ëŠ”ë°, latent representationëŠ” high-frequency detailì„ ì—†ì• ë©´ì„œ ì´ë¥¼ í•™ìŠµ(?) ì´ê²Œ ì •í™•íˆ ë­ì§€
    - Super resolution, Inpainting task

* **DallE 2(unCLIP)**: 2022. 4 | OpenAI
    - CLIP Objectiveë¥¼ í†µí•´ ì£¼ì–´ì§„ text query & image queryì— ë§ì¶°ì„œ image / text encoderë¥¼ í•™ìŠµí•˜ê³ , generation processì—ì„œëŠ” CLIP Text embeddingì„ í†µí•´ priorë¥¼ ë§Œë“¤ì–´ì¤€ í›„ diffusion decoderì— ë„£ì–´ì£¼ëŠ” í˜•íƒœë¡œ generationì„ ì§„í–‰í•œë‹¤.
    - CLIP Encoder + Diffusion Decoder êµ¬ì¡°
        - ì¼ë°˜ì ìœ¼ë¡œ deterministicí•œ decoderì™€ ë‹¤ë¥´ê²Œ non-deterministicí•˜ê²Œ ëŒì•„ê°„ë‹¤ (stochastic termì´ ìˆëŠ” ê²ƒ ê°™ë‹¤). ì´ non-deterministicí•œ ì„±ì§ˆì´ deterministicí•œ GANë“¤ì´ ë¹„ìŠ·í•œ ì´ë¯¸ì§€ë¥¼ ë‚´ë±‰ëŠ” ê²ƒê³¼ ë‹¤ë¥´ê²Œ êµ‰ì¥íˆ ë‹¤ì–‘í•œ ì´ë¯¸ì§€ë¥¼ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.
    - Encoder: Image embedding with text prompt -> ì„ë² ë”©ì„ ë‚´ë†“ìœ¼ë©´ decoderí•œí…Œ text promptë‘ ê°™ì´ ë„˜ê²¨ì„œ generateí•˜ëŠ” êµ¬ì¡°
        - ì—¬ê¸°ì„œ encoderê°€ image embeddingì„ í•˜ëŠ” ê³¼ì •ì´ ë‘ ê°€ì§€ë¡œ ë‚˜ë‰œë‹¤: AR vs. Diffusion
            - AR: Text-encoding + Text Embedding + Time Embedding + Image Embedding + Learned Queriesë¥¼ ì‹œì‘ì ìœ¼ë¡œ Auto-regressiveí•˜ê²Œ z_ië¥¼ ì¶”ë¡ í•˜ê³  l2ë¡œ í•™ìŠµ
            - Diffusion: ì–»ì–´ë‚¸ Image embeddingì„ LDM í˜•ì‹ìœ¼ë¡œ í•™ìŠµ - image embeddingì„ normalë¡œ ë§Œë“œëŠ” forward process ê·¸ë¦¬ê³  ì›ë³µí•˜ëŠ” í˜•íƒœë¡œ ë§Œë“¤ì–´ì£¼ê³  ì—¬ê¸°ì— conditioningì„ í•˜ë©´ì„œ ì´ë¯¸ì§€ê°€ 

* **Imagen**: 2022. 5 | Google Research
    - LMì˜ ì—­í• ì´ ë§¤ìš° ì¤‘ìš”í–ˆë‹¤! **Frozen Pre-trained LLMìœ¼ë¡œ íŠœë‹í•œ ì²« ì‚¬ë¡€**ë¡œ ë´ì•¼í• ë“¯
    - Classifier guidanceì˜ weightë¥¼ **dynamic thresholding**ì„ ì ìš©
        - Noise predictioní•œ ì´ë¯¸ì§€ì˜ absolute valueë¡œë¶€í„° percentileì„ êµ¬í•´ì„œ ê·¸ ê°’ì„ í™œìš©: ì´ê²Œ ì–´ë–»ê²Œ ìœ ì˜ë¯¸í•œì§€ ì˜ëª¨ë¥´ê² ë‹¤.
    - **Efficient U-Net** êµ¬ì¡°: ì œì¼ í° ê±´ self-attentionì´ ì‚¬ë¼ì§„ ê²ƒ ê°™ë‹¤. textì™€ì˜ cross-attentionì€ ë‚¨ê²¨ë‘ì—ˆë‹¤.
    - **DrawBench Dataset** ê³µê°œ

* **Parti**: 2022. 6 | Google Research
    - Pathways AutoRegresstive Text-to-Image Modelsì˜ ì¤„ì„ë§ë¡œ seq2seq í˜•íƒœë¡œ ë¬¸ì œë¥¼ í‘¼ë‹¤. ë³´í†µ NMTì—ì„œ í•œ ì–¸ì–´ë¡œë¶€í„° ë‹¤ë¥¸ ì–¸ì–´ë¡œ ë²ˆì—­í•˜ëŠ” í˜•ì‹ì´ë¼ë©´, text-promptë¥¼ source sequence, image tokenì„ target sequenceë¡œ ë†“ê³  í’€ì—ˆë‹¤ëŠ” ì .
    - Encoder-decoder style: Decoder-only í˜•íƒœ (ë³´í†µ text-image concatí•˜ê³  í•˜ë‚˜ì”© ARë¡œ ë½‘ì•„ë‚´ëŠ” êµ¬ì¡°)ì—ì„œ ë³€í™”ë¥¼ ê°€í–ˆë‹¤. textëŠ” encode, imageëŠ” encoded textë¥¼ KVë¡œ ë°›ì•„ ARí˜•íƒœë¡œ ë½‘ì•„ë‚´ëŠ” êµ¬ì¡° (ë”± seq2seq)
    - PartiPrompt: 1600ê°œì˜ promptë¡œ ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ì™€ ëª¨ë¸ì˜ ë¬¼ì²´ ì´í•´ë ¥ì„ í‰ê°€í•  ìˆ˜ ìˆëŠ” í”„ë¡¬í”„íŠ¸ê°€ í¬í•¨ë˜ì–´ ìˆë‹¤.
* **EDM (Denoise Score Matching)**: 2022. 10 (NeurIPS 2022) | NVIDIA
* **Variational Diffusion Models**: 2022. 12 | Google Research
* **MUSE**: 2023. 1 | Google Research
    - VQGANì˜ discrete tokenìœ¼ë¡œ ì¡°ê¸ˆ ë” efficientí•˜ê²Œ ë½‘ì•„ë‚¼ ìˆ˜ ìˆë‹¤.
    - Parallel decoding ë˜í•œ í™œìš©í–ˆë‹¤ëŠ”ë° ì½”ë“œê°€ ì—†ì–´ì„œ ì˜ ëª¨ë¥´ê² ë‹¤. DETR ë§ê³ ëŠ” ë‚´ê°€ ì•„ëŠ” ì‚¬ë¡€ê°€ ì—†ì–´ì„œ ì•Œê¸°ê°€ ì–´ë µë‹¤.
* **ControlNet**: 2023. 2 | Stanford

* **Consistency Models**: 2023. 3 | OpenAI

# Generative Metrics
### Inception Score
* Multiplication of **Sharpness (S)** and **Diversity (D)**, obtained by Inception model. This is somewhat correlated to human evaluation but may not cover the whole thing.

| **Inception Score (IS)** was proposed by Salimans et al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS [3].

### Frechet Inception Distance (FID)
* Caclulates **feature space distance** with Frechet Distance. FD calculates two normal distributions via sum of mean difference square and std difference square. This does NOT take how synthetic images compare to real images into account.
* [Reference: How to Evaluate GANs using FID](https://wandb.ai/ayush-thakur/gan-evaluation/reports/How-to-Evaluate-GANs-using-Frechet-Inception-Distance-FID---Vmlldzo0MTAxOTI)

| To better capture diversity than IS, **FrÃ©chet Inception Distance (FID)** was propose judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 [62] latent space. Recently, sFID was proposed by Nash et al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure

### Precision and Recall | 2019.4 NVIDIA
| Finally, KynkÃ¤Ã¤nniemi et al. [32] proposed **Improved Precision and Recall metrics** to separately measure sample fidelity as the fraction of model samples which fall into the _data manifold_ (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall).

### CLIP Score

### CAS: Classification Accuracy Score
Train ResNet-50 classifier soley on samples generated by the candidate model, and then measuring the classifier's classification accuracy on the ImageNet validation set.


# Generative Loss
* Perceptual Loss


# Research Groups
* CompVis Group
    - Computer Vision and Learning research group at Ludwig Maximilian University of Munich (formerly Computer Vision Group at Heidelberg University)
    - Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patric Esser, Bjorn Ommer

* Google Research
    - Jonathan Ho
    - Tim Salimans
    - Christian Saharia
    - Ben Poole
    - Diederiek P. Kingma
    - Jarred Barber
    - Yang Song
    - Abhishek Kumar
    - Jascha Sohl-Dickstein
    - Diederik P. Kingma

* OpenAI
    - Prafulla Dhariwal
    - Radford Alec
    - Jong Wook Kim
    - Chris Hallacy
    - Gabriel Goh
    - Aditya Ramesh
    - Alex Nichol

* NVIDIA
    - Karras Tero
    - Miika Aittala
    - Timo Aila
    - Samuli Laine

* Stanford
    - Jiaming Song
    - Chenlin Meng
    - Stefano Ermon
{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ubuntu-setup","text":"<p>This wiki is about tricks afronted while doing deep learnings on server environments</p>"},{"location":"#1-about-python-tricks","title":"1. About python tricks","text":""},{"location":"#2-about-linux-tricks","title":"2. About linux tricks","text":""},{"location":"#3-about-docker-tricks","title":"3. About docker tricks","text":""},{"location":"#4-about-deep-learning-implementations","title":"4. About deep learning implementations","text":""},{"location":"01_python/0_index/","title":"Python Setups","text":""},{"location":"01_python/0_index/#1-install-anaconda","title":"1. Install Anaconda","text":""},{"location":"01_python/1_conda/","title":"Anaconda installation","text":"<ul> <li>Instructions <pre><code>installsh=Anaconda3-2023.03-Linux-x86_64.sh\nwget https://repo.anaconda.com/archive/${installsh}\nshasum -a 256 ${installsh}\nbash ${installsh}\n</code></pre></li> </ul>"},{"location":"01_python/troubleshoots/0_index/","title":"Python Troubleshoots","text":"<p>Collections of shitty python troubleshoots and tricks</p>"},{"location":"01_python/troubleshoots/pytorch/buffer/","title":"<code>torch.register_buffer</code> vs <code>torch.Parameter</code>","text":"<ul> <li><code>buffer</code><ul> <li>Registered in model but NOT optimized</li> </ul> </li> <li><code>torch.Parameter</code><ul> <li>Being trained.</li> </ul> </li> </ul> <p>Reference</p>"},{"location":"01_python/troubleshoots/pytorch/imgaug/","title":"<code>imgaug</code>","text":"<p>Install <code>imgaug</code> from https://github.com/marcown/imgaug. The main package no longer develops </p>"},{"location":"02_linux/1_zsh/","title":"<code>zsh</code>","text":"<p>Why <code>zsh</code>? - <code>zsh</code> is more capable of auto-completion, boosting productivity for developers.</p>"},{"location":"02_linux/1_zsh/#installing-zsh","title":"Installing <code>zsh</code>","text":"Install zsh<pre><code>sudo apt-get update # (1)\nsudo apt install zsh\n</code></pre> <ol> <li>:man_raising_hand: \uc790\uc8fc\ud560\uc218\ub85d \uc794 \uc5d0\ub7ec\uac00 \uc548\uc0dd\uae40</li> </ol>"},{"location":"02_linux/1_zsh/#zsh-as-default-shell","title":"<code>zsh</code> as default shell","text":"<pre><code>chsh -s $(which zsh)\n</code></pre>"},{"location":"02_linux/1_zsh/#pretty-zsh","title":"Pretty ZSH","text":"<pre><code>sh -c \"$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\ngit clone https://github.com/sindresorhus/pure.git ~/.zsh/pure\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ~/.zsh/zsh-syntax-highlighting\ngit clone https://github.com/zsh-users/zsh-autosuggestions ~/.zsh/zsh-autosuggestions\n</code></pre> <ul> <li>Append the followings to <code>~/.zshrc</code> file.</li> <li><code>source activate</code> may not work if conda is not installed. After installing anaconda, the command will not provoke an error <pre><code>path+=/opt/conda/bin\nTZ=Asia/Seoul\nLANG=C.UTF-8\nLC_ALL=C.UTF-8\nPYTHONDONTWRITEBYTECODE=1\nPYTHONUNBUFFERED=1\nPYTHONIOENCODING=UTF-8\nPYTHONHTTPSVERIFY=0\nfpath+=(\"$HOME/.zsh/pure\")\\nautoload -U promptinit; promptinit\\nprompt pure\nsource ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\nsource ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh\nZSH_AUTOSUGGEST_HIGHLIGHT_STYLE='fg=111'\nsource activate\n</code></pre></li> </ul>"},{"location":"02_linux/2_ssh/","title":"<code>ssh</code> Connection","text":""},{"location":"02_linux/2_ssh/#1-starting-ssh","title":"1. Starting <code>ssh</code>","text":"<pre><code>sudo apt-get update\nsudo apt-get install openssh-server\nsudo systemctl restart sshd\n</code></pre>"},{"location":"02_linux/2_ssh/#2-ssh-without-passwd-use-pub-keys","title":"2. <code>ssh</code> without passwd: use pub keys","text":""},{"location":"02_linux/2_ssh/#1-create-a-new-key","title":"(1) Create a New Key","text":"<pre><code>cd ~/.ssh\nssh-keygen -t rsa -f id_rsa\nls\n&gt;&gt;&gt; id_rsa id_rsa.pub\n</code></pre>"},{"location":"02_linux/2_ssh/#2-send-id_rsapub-to-remote-server","title":"(2) Send <code>id_rsa.pub</code> to remote server","text":"<p>Send public key to remote server <pre><code>scp -P 22 id_rsa.pub (user)@(IP):(ABS_PATH)\n</code></pre></p>"},{"location":"02_linux/2_ssh/#3-edit-keys-at-your-remote-server","title":"(3) Edit keys at your remote server","text":"<pre><code>ssh (user)@(IP) -p 22\nmkdir ~/.ssh # ~/.ssh \ub514\ub809\ud1a0\ub9ac\uac00 \uc5c6\ub294 \uacbd\uc6b0\nchmod 700 ~/.ssh\n\ncat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 600 ~/.ssh/authorized_keys\n</code></pre>"},{"location":"02_linux/2_ssh/#4-upload-new-ip-on-config","title":"(4) Upload new IP on Config","text":"<p>Where is ssh configuration?</p> <ul> <li>In general ssh configuration file <code>config</code> stays in your <code>~/.ssh/</code> directory.</li> <li>Basic setup as follows</li> <li>Put your <code>id_rsa</code> key name in <code>IdentityFile</code></li> </ul> <pre><code>Host (HOSTNAME)\n  HostName (IP)\n  User (USERNAME)\n  Port (PORT-NUMBER)\n  IdentityFile ~/.ssh/id_rsa\n</code></pre>"},{"location":"02_linux/2_ssh/#proxy-jumps","title":"Proxy Jumps","text":"<ul> <li>Reference <pre><code>Host HostA\n  HostName hostA\n  User userA\n\nHost HostB\n  HostName hostB\n  User userB\n  ProxyJump HostA\n</code></pre></li> <li>One can add <code>IdentityFile</code> on final target server <code>HostB</code></li> </ul> <p>Who needs proxy?</p> <ul> <li>Some servers are not accessible even with VPN</li> <li>lab-2080 server is the only available server access thorugh VPN and other servers can be accessed via lab-2080 server.</li> <li>Also, VSCode IDE is available for non-accessible servers with VPN with proxy jump settings.</li> </ul>"},{"location":"02_linux/3_gpu/","title":"Nvidia setups for linux","text":"<ul> <li>Follow the official instructions from nvidia. Below 3 paragraphs are most essential.<ul> <li>2. Pre-install</li> <li>3.10 Ubuntu Package Manager Installation</li> <li>4. Driver installation</li> </ul> </li> <li> <p>This worked for me (Mar 22, 2023), by follwoing the instructions <pre><code>sudo apt install cuda-drivers-525\nlspci | grep -i nvidia\nuname -m &amp;&amp; cat /etc/*release\nsudo apt-get install linux-headers-$(uname -r)\nsudo apt autoremove\nsudo apt-get install linux-headers-$(uname -r)\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb\nsudo dpkg -i cuda-keyring_1.0-1_all.deb\nsudo apt-get update\nsudo apt-get -y install cuda\nsudo dpkg -i cuda-keyring_1.0-1_all.deb\nsudo apt-key del 7fa2af80\nsudo apt-get update\nsudo apt-get install cuda\nsudo apt-get install nvidia-gds\nsudo reboot\nsudo apt-get install cuda-drivers-525\nsudo reboot\nnvidia-smi\n</code></pre></p> </li> <li> <p>Use <code>nvidia-smi</code> to check gpu utilities</p> </li> </ul>"},{"location":"02_linux/3_gpu/#fixing-updating-nvidia-smi","title":"Fixing / Updating nvidia-smi","text":"<p>Sometimes <code>nvidia-smi</code> command does not work with following error. NVIDIA-SMI Error<pre><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n</code></pre></p> <p>Fix with following command <pre><code>sudo apt install --fix-broken\n</code></pre> Reference</p>"},{"location":"02_linux/3_gpu/#power-limit","title":"Power Limit","text":"<p>Power limit is to restrict maximum power of server. <pre><code>sudo nvidia-smi -i 0 -pl 330\n</code></pre></p> <p>Currently, for 245 server we set down power limit from 390W to 330W.</p>"},{"location":"02_linux/4_github/","title":"GitHub (<code>gh</code>)","text":"<p><code>gh</code> is a command to authenticate/login to your account. Basic <code>git</code> command is different from <code>gh</code>.</p>"},{"location":"02_linux/4_github/#install-gh","title":"Install <code>gh</code>","text":"<p>Installing gh. One may need 'sudo su' to make it happen<pre><code>type -p curl &gt;/dev/null || sudo apt install curl -y\ncurl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \\\n&amp;&amp; sudo chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \\\n&amp;&amp; echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list &gt; /dev/null \\\n&amp;&amp; sudo apt update \\\n&amp;&amp; sudo apt install gh -y\n</code></pre> Upgrade gh<pre><code>sudo apt update\nsudo apt install gh\n</code></pre></p>"},{"location":"02_linux/4_github/#github-login-through-cli","title":"GitHub Login through CLI","text":"<p>Then login through <code>gh</code>. If you're running this on your server, choose \"Paste an authentication token\" on third choice, and paste your developer personal access tokens. <pre><code>gh auth login\n</code></pre></p> <p>What is <code>gpg keys</code>?</p> <p>GitHub no longer allows you to type in passwords on CLI, which means no one can clone/pull/commit anything on cli through passwords. Instead, everybody uses Personal access tokens. Lookup here for more details</p>"},{"location":"02_linux/02_03_commands/10_dmseg/","title":"Debugging Kernel","text":"<p>\uc885\uc885 \uc791\uc5c5 \uac78\uc5b4 \ub193\uc740 \ucf54\ub4dc\uac00 \ud655\uc778\ud574\ubcf4\uba74 <code>Process killed</code> \ub2f9\ud588\ub2e4\uace0 \ub730 \ub54c\uac00 \uc788\ub2e4. \uc774\ub7f4 \ub54c \uc65c \uc774\uac8c kill \ub2f9\ud588\ub294\uc9c0 \uc54c\uc544\ubcf4\ub294 \ubc29\ubc95\uc73c\ub85c\ub294 <code>dmseg</code>\uac00 \uc788\ub2e4. </p>"},{"location":"02_linux/02_03_commands/1_watch/","title":"watch","text":""},{"location":"02_linux/02_03_commands/1_watch/#watch-to-monitor-something","title":"<code>watch</code>: To monitor something","text":"<p>For gpu monitoring <pre><code>watch -d -n 0.5 nvidia-smi\n</code></pre></p> <ul> <li><code>-d</code>: Show difference</li> <li><code>-n</code>: Time interval</li> </ul>"},{"location":"02_linux/02_03_commands/2_tmux/","title":"tmux","text":""},{"location":"02_linux/02_03_commands/2_tmux/#tmux-running-in-background-but-accessible","title":"<code>tmux</code>: Running in background, but accessible","text":"Installing tmux<pre><code>sudo apt-get install tmux\n</code></pre>"},{"location":"02_linux/02_03_commands/2_tmux/#basic-usage","title":"Basic usage","text":"<ul> <li><code>tmux</code> consists of session - pane hierarchy. <code>work</code> is the name of session.</li> <li>New session: <code>tmux new -s work</code></li> <li>Exit session: CTRL B, D</li> <li>Resume to session: <code>tmux attach -t work</code></li> <li>Divide session into pane:<ul> <li>Horizontal spilt: CTRL B, \"</li> <li>Vertical split: CTRL B, %</li> </ul> </li> <li>Entering Command mode: CTRL B, :<ul> <li>Removing panes: <code>kill-pane -t (index)</code>, where index counts from 0</li> </ul> </li> </ul>"},{"location":"02_linux/02_03_commands/3_rsync/","title":"rsync","text":""},{"location":"02_linux/02_03_commands/3_rsync/#rsync-send-files-to-remote-server","title":"<code>rsync</code>: Send files to remote server","text":"<p>Basic rsync usage<pre><code>rsync -e 'ssh -p 20000' LOCALDIR REMOTEDIR\n</code></pre> Flags</p> <ul> <li><code>-v</code>: verbosity</li> <li><code>-r</code>: recursive</li> <li><code>-a</code>: archive mode</li> <li><code>-z</code>: Data compress</li> <li><code>-h</code>: Human-readable</li> <li><code>-e</code>: Extra ssh flags to feed</li> </ul> <p>Sometimes permission issue may be raised. Use <code>chmod -R 777 (TARGET_DIR)</code> to add permission</p> <p>Reference</p> <ul> <li>Rsync basics</li> <li>Changing ports</li> </ul>"},{"location":"02_linux/02_03_commands/4_storage/","title":"storage","text":""},{"location":"02_linux/02_03_commands/4_storage/#checking-storage-for-certain-directory","title":"Checking storage for certain directory","text":"<pre><code>ls | xargs du -sh\n</code></pre> <p>What is <code>xargs</code>?</p> <p>Sometimes you want outputs from a certain command such as list, and feed them to other commands. This is available through <code>xargs</code>. Above command allows <code>ls</code> stdout to be stdin of <code>du -sh</code> command.</p>"},{"location":"02_linux/02_03_commands/5_mount/","title":"mount","text":""},{"location":"02_linux/02_03_commands/5_mount/#mounting-external-disk","title":"Mounting External disk","text":"Mounting external disk<pre><code>sudo fdisk -l\nsudo mkfs.ext4 /dev/sda # (1)\nsudo mkdir /mnt/hdd\nsudo mount /dev/sda /mnt/hdd\n</code></pre> <ol> <li>Please check your disk name: <code>dev/sda</code>. This command formats hard disk.</li> </ol> <p>What is <code>ext4</code>?</p> <p>The <code>mkfs</code> command is used in Linux and Unix operating systems to create a file system on a storage device. The <code>-t</code> option specifies the type of file system to be created. <code>ext2</code>, <code>ext3</code>, and <code>ext4</code> are all different versions of the Linux file system.  - <code>ext2</code> is the second extended file system and was the default file system for Linux for many years. It does not support journaling, which means that it can take longer to recover data after a crash or power failure.  - <code>ext3</code> is an extension of the <code>ext2</code> file system and includes journaling. Journaling helps to reduce the time it takes to recover data after a crash or power failure. It is backward-compatible with <code>ext2</code>, which means that you can mount an <code>ext2</code> file system as an <code>ext3</code> file system.  - <code>ext4</code> is the fourth extended file system and includes improvements over <code>ext3</code>. It has faster file system checks and supports larger file systems and files. It also includes a number of other features, such as delayed allocation and multiblock allocation, which improve performance and reduce fragmentation.  When using the <code>mkfs</code> command with the <code>-t</code> option, you should specify the appropriate file system type for your needs. For example, if you are creating a file system on a large storage device with large files, <code>ext4</code> may be a good choice.</p> <p>\ubd80\ud305 \uc2dc \uc790\ub3d9\uc73c\ub85c \uc801\uc6a9\ub418\uac8c \ud558\ub824\uba74 <code>/etc/fstab</code>\uc5d0 \ub123\uc5b4\uc918\uc57c\ud55c\ub2e4. \ub123\uc5b4\uc904 \ub54c\ub294 \uc544\ub798\uc758 \uad6c\uc131\uc694\uc18c\ub4e4\uc744 \uac16\ucdb0\uc57c\ud55c\ub2e4. <pre><code>UUID=&lt;UUID&gt; &lt;mount_point&gt; &lt;file_system_type&gt; defaults 0 2\n\n# e.g.\nUUID=6D2B-0058 /mnt/my_external_drive ext4 defaults 0 2\n</code></pre></p> <ol> <li>How to check <code>&lt;UUID&gt;</code>?: <code>sudo blkid</code> and check file systems UUID</li> <li>What is <code>&lt;mount point&gt;</code>?: Mount point is the directory that you have mounted with <code>mount</code> command</li> <li>What is <code>&lt;file_system_type&gt;</code>?: </li> <li>What are <code>defaults 0 2</code>?: The defaults option specifies the default mount options, and the 0 2 at the end specifies the dump and file system check options. You can leave these as they are for most cases.</li> </ol> <p>What is <code>0 2</code> and <code>0 0</code>?</p> <p>In the /etc/fstab file, the 0 2 and 0 0 at the end of each line specify the dump and file system check options for the corresponding file system.  The first number (dump) specifies whether or not the file system should be backed up using the dump command. A value of 0 means that the file system should not be backed up, while a value of 1 means that it should.  The second number (file system check) specifies the order in which the file system check should be performed at boot time. A value of 0 means that the file system should not be checked, while a value of 1 means that it should be checked first. The value 2 means that it should be checked after all file systems with a value of 1 have been checked.  In general, the dump option is not used anymore and is usually set to 0. The file system check option is more important, as it determines the order in which file systems are checked for errors during the boot process.  In your case, if a file system in your /etc/fstab file has 0 0 instead of 0 2, it means that the file system will not be checked for errors during the boot process. This might be appropriate for some types of file systems that are known to be very stable and reliable, such as read-only file systems or file systems on removable media.  However, it's generally a good idea to have all file systems checked for errors during the boot process. If you want to change the file system check option for a particular file system in /etc/fstab, you can edit the corresponding line and change the 0 0 to 0 2.</p>"},{"location":"02_linux/02_03_commands/6_files/","title":"Files","text":""},{"location":"02_linux/02_03_commands/6_files/#count-number-of-files","title":"Count number of files","text":"<pre><code>ls | wc -l\n</code></pre> <p>What is <code>|</code>?</p> <p><code>|</code> works as a pipe. This operand will take outputs from previous command and send it to next command. From above example, <code>wc -l</code> will count the number of outputs from <code>ls</code>.</p>"},{"location":"02_linux/02_03_commands/6_files/#removing-certain-files","title":"Removing certain files","text":"For certain files not directories<pre><code>find . -type f -name '*.o'\nfind . -type dir -name '*.o'\n\n# Remove certain files\nfind . -type f -name '*.o' | xargs rm -r\nfind . -type f -name '*.o' -delete\n</code></pre> <p>For some reasons, this does not delete Reference</p>"},{"location":"02_linux/02_03_commands/7_users/","title":"Ubuntu Users","text":""},{"location":"02_linux/02_03_commands/7_users/#add-user","title":"Add User","text":"<p>Add user<pre><code>adduser 1pha\nsudo usermod -aG sudo 1pha\n</code></pre> This is simply adding a user into <code>sudo</code> group. If one account requires belonging to a <code>docker</code> group, to avoid typing <code>sudo</code> in every commands, simply <code>sudo usermod -aG docker 1pha</code>.</p>"},{"location":"02_linux/02_03_commands/7_users/#checking-out-to-other-user","title":"Checking out to other User","text":"checkout<pre><code>su 1pha\n</code></pre>"},{"location":"02_linux/02_03_commands/8_zip/","title":"Compress &amp; Unzip Files","text":""},{"location":"02_linux/02_03_commands/8_zip/#zip","title":"<code>zip</code>","text":"<p>Note that it's a good convention to <code>.zip</code> extension in <code>(FILENAME)</code> Compress with zip<pre><code>zip (FILENAME) (TARGET_DIR)\n</code></pre></p> <p>If <code>(TARGET_DIR)</code> contains subdirectory, than recursive flag of <code>-r</code> is required. Compress with zip with subdirectories<pre><code>zip -r (FILENAME) (TARGET_DIR)\n</code></pre></p> Unzip<pre><code>unzip (FILENAME)\nunzip (FILENAME) -d (TARGET_DIR)\n</code></pre>"},{"location":"02_linux/02_03_commands/8_zip/#tar","title":"<code>tar</code>","text":"<p>Flags * <code>-z</code>: Compress with gzip(gz) * <code>-j</code>: Compress with bzip2(bz) * <code>-c</code>: collect to tar * <code>-x</code>: extract tar * <code>-f</code>: (Mandatory) Collects file * <code>-v</code>: Verbosity * <code>-p</code>: Compress with ownership</p> <p>What is the difference between <code>.tar</code> and <code>.tar.gz</code>?</p> <p><code>.tar</code> does NOT compress directories but rather collects all files to archive. Compressed <code>.tar</code> file becomes <code>.tar.gz</code>. This uses gunzip to compress files.</p> <p><code>(FILENAME)</code> contains <code>.tar</code> Compress and decompress to tar<pre><code># Compress\ntar -cvf (FILENAME) (TARGETDIR)\n\n# Decompress\ntar -xvf (FILENAME)\n</code></pre></p> Compress and decompress to tar.gz<pre><code># Compress\ntar -zcvf (FILENAME) (TARGETDIR)\n\n# Decompress\ntar -zxvf (FILENAME)\n</code></pre>"},{"location":"02_linux/02_03_commands/9_stats/","title":"How to check Resource Metrics","text":""},{"location":"02_linux/02_03_commands/9_stats/#ram","title":"RAM","text":"Checking RAM<pre><code>free -mh\n</code></pre>"},{"location":"02_linux/02_03_commands/9_stats/#monitoring","title":"Monitoring","text":"<p><pre><code>top\n</code></pre> <pre><code>htop\n</code></pre></p>"},{"location":"03_dl/","title":"Deep Learning","text":"<p>Deep learning model implementation will be posted here.</p>"},{"location":"03_dl/0_basics/1_attention/","title":"Basic Attention Codes","text":"Attention<pre><code>class Attention(nn.Module):\n    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n        super().__init__()\n        self.num_heads = num_heads\n        head_dim = dim // num_heads\n        self.scale = qk_scale or head_dim ** -0.5\n\n        # \ud55c \ubc88\uc5d0 \uc785\ub825\uc744 q, k, v\ub85c \ub123\uace0 \ucabc\uac2c\n        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x):\n        B, N, C = x.shape\n        # \uc774\ub7f0 \ubcf5\uc7a1\ud55c \ubc29\uc2dd\uc744 \ud53c\ud558\uae30 \uc704\ud574 einops\ub97c \uc0ac\uc6a9\ud558\uae30\ub3c4\ud568\n        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        attn = (q @ k.transpose(-2, -1)) * self.scale\n        attn = attn.softmax(dim=-1)\n        attn = self.attn_drop(attn)\n\n        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n        x = self.proj(x)\n        x = self.proj_drop(x)\n        return x, attn\n</code></pre>"},{"location":"03_dl/0_basics/2_2_swin/","title":"Swin","text":""},{"location":"03_dl/0_basics/2_2_swin/#swin_v2","title":"Swin_v2","text":"<p>TL;DR</p> <ul> <li>Large-scale ViT in computer vision<ul> <li>Residual-post-norm + cosine attention</li> <li>Log-spaced continuous position bias: \uc791\uc740 \uc774\ubbf8\uc9c0 pre-train -&gt; \ud070 \uc774\ubbf8\uc9c0\uc5d0 fine-tune\ud560 \ub54c \uc0dd\uae38 \uc218 \uc788\ub294 relative positional bias\uc758 \uac12 \ud06c\uae30 \ucc28\uc774\ub85c \uc778\ud55c \uc131\ub2a5 \uc800\ud558\ub97c \ub9c9\uae30 \uc704\ud574 positional bias\uc5d0 log\ub9cc \ucd94\uac00\ud588\uc74c</li> <li>\ud559\uc2b5\ud560 \ub54c SSL \ubc29\ubc95\uc744 \uc774\uc6a9\ud588\uc74c</li> </ul> </li> <li>\uc5ec\ub7ec \uc2e4\ud5d8\uc744 \ud1b5\ud574 \uc704\uc5d0 \ub300\ud55c \uadfc\uac70\ub4e4\uc744 \uc815\ub9ac</li> </ul>"},{"location":"03_dl/0_basics/2_2_swin/#introduction","title":"Introduction","text":"<ol> <li> <p>Instability Issue</p> <ul> <li>\ubaa8\ub378\uc758 \uaddc\ubaa8\uac00 \ucee4\uc9c8\uc218\ub85d \ud6c4\ubc18 \ub808\uc774\uc5b4\uc758 activation\uac12\uc774 \ub108\ubb34 \ucee4\uc9c0\uac8c \ub41c\ub2e4 (Figure 2)</li> <li></li> <li>Suggestion<ul> <li>Residual post-normalization</li> <li>Scaled Cosine Attention</li> </ul> </li> </ul> </li> <li> <p>Image Resolution Issue</p> <ul> <li>Pre-trained \ubaa8\ub378\uc774 \ud559\uc2b5\ud588\ub358 resolution\uacfc \ub2e4\ub978 resolution\uc744 \ub123\uc5b4\uc11c \ud559\uc2b5\ud558\uba74 \uc131\ub2a5\uc774 \ud06c\uac8c \uc800\ud558\ub418\ub294 \uc810</li> <li>\uc774\ub294 Relative positional embedding\uc774 \ub108\ubb34 \uac12\uc774 \ud06c\uac8c \ub4e4\uc5b4\uac00\uae30 \ub54c\ubb38\uc5d0 resolution\uc774 \uc624\ub974\uba74 \uc774 embedding\uac12\uc774 vector embedding\uc5d0 \ub9ce\uc740 \uc601\ud5a5\uc774 \uc788\uae30 \ub54c\ubb38\uc73c\ub85c \ud310\ub2e8</li> <li>Suggestion<ul> <li>Log-spaced continuous position bias</li> </ul> </li> </ul> </li> </ol>"},{"location":"03_dl/0_basics/2_vit/","title":"Basic Vision Transformer Block","text":"Patch Embedding<pre><code>class PatchEmbed(nn.Module):\n    \"\"\" Image to Patch Embedding\n    \"\"\"\n    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\n        super().__init__()\n        num_patches = (img_size // patch_size) * (img_size // patch_size)\n        self.img_size = img_size\n        self.patch_size = patch_size\n        self.num_patches = num_patches\n\n        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n\n    def forward(self, x):\n        B, C, H, W = x.shape\n        x = self.proj(x).flatten(2).transpose(1, 2)\n        return x\n</code></pre>"},{"location":"03_dl/1_diffusions/tldr/","title":"Brief History of Diffusion-based Generative models and their variants","text":"<ul> <li>BigGAN: 2019. 2 (ICLR 2019) | DeepMind</li> <li>Generative Modeling by Estimating Gradients of the Data Distribution: 2019. 7 | Stanford</li> <li>DF-GAN: 2020. 8<ul> <li>Text-to-Image</li> </ul> </li> <li>\ud83d\udccc DDPM: 2020. 12 (NeurIPS 2020) | Berkeley \u2192 Google Research</li> <li>DDIM: 2020. 10 | Stanford<ul> <li>\uae30\uc874 DDPM\uc774 Noising process\uac00 Markovian\uc774\ub77c \ubc14\ub85c \uc774\uc804\uc758 timestep noised image\uc5d0\ub9cc \uc758\uc874\ud558\uc5ec \uc804\uccb4 \ud504\ub85c\uc138\uc2a4\ub97c \ub9cc\ub4e4\uc5b4\ub0b4\ub294\ub370 \uc2dc\uac04\uc774 \ub9ce\uc774 \uac78\ub9b0\ub2e4. \uc5ec\uae30\uc11c Non-markovian\uc73c\ub85c $x_0$\uc5d0\uc11c $x_T$\ub85c \uac00\ub294 \uacfc\uc815\uc744 deterministic\ud558\uac8c forward process\ub97c \uc77c\ubd80\ub9cc sampling\ud558\uc5ec train\ud558\uace0 \ucd94\ub860\ud560 \ub54c\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\ub85c \uc804\uccb4 step\uc744 \ub2e4 \uac70\uce58\uc9c0 \uc54a\uc544 accelerated inference\uac00 \uac00\ub2a5\ud558\ub2e4.</li> </ul> </li> <li>Score-based Generative Modeling through Stochastic Differential Equations: 2020. 11 | Google Research</li> <li> <p>VQGAN: 2020. 12 (CVPR 2021) | CompVis</p> <ul> <li>ConvNet\uacfc Transformer\uac00 generation task\uc5d0\uc11c \uac01\uac01 \uc798\ud558\ub294 \ubd80\ubd84\uc744 \ub2e4 \uc0ac\uc6a9\ud574\uc11c image tokenization\uc744 \uc9c4\ud589<ul> <li>ConvNet\uc740 inductive bias\ub85c \uc778\ud574 local \uc815\ubcf4\ub97c \ub9ce\uc774 \uac00\uc9c0\uace0 \uc788\ub2e4. \ud558\uc9c0\ub9cc global \uc815\ubcf4\ub97c \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\ub2e4.</li> <li>Transformer model\uc740 low inductive bias \ub355\ubd84\uc5d0 expressive\ud558\ub2e4 (\uc6cc\ub529 \ub54c\ubb38\uc5d0 \ud63c\ub780\uc2a4\ub7ec\uc6b4\ub370 \uc544\ub9c8 global information\uc744 \ubcf4\uc720\ud55c\ub2e4 \uc815\ub3c4\ub85c \uc774\ud574\ud558\uba74 \ub418\uc9c0 \uc54a\uc744\uae4c \uc2f6\ub2e4). \ud558\uc9c0\ub9cc \uace0\ud654\uc9c8\uc758 \uc774\ubbf8\uc9c0\ub97c \ub9cc\ub4e4\uae30\ub294 \uc5b4\ub835\ub2e4.</li> </ul> </li> <li>ConvNet\uc73c\ub85c image feature map\uc744 \ub9cc\ub4e0 \ud6c4 \uc774\ub97c quantize\ud558\uc5ec latent image code\ub97c \ub9cc\ub4e0\ub2e4. \ub610\ud55c \uc774 code\ub97c decoder\uc5d0 \ud0dc\uc6cc GAN \uad6c\uc870\ub85c \ud559\uc2b5\uc744 \uc2dc\ud0a8\ub2e4.</li> <li>\uadf8 \ud6c4 image\ub97c tokenize\ud558\uc5ec sequence\ub85c \ubcf4\uc720\ud55c \ud6c4 \uc774\ub97c language model\ucc98\ub7fc \ud559\uc2b5\uc2dc\ud0a8\ub2e4 (Auto-regressive\ud558\uac8c next token prediction \uac00\uae4c\uc6c0)</li> <li>Auto-regressive\ud55c \ud2b9\uc131\uc744 \ud65c\uc6a9\ud558\uc5ec conditioning\uc744 \ud560 \uc218 \uc788\ub2e4: \uc2dc\uc791\uc810\uc5d0 conditioning information\uc744 prepend\ud574\uc11c \ucd94\ub860</li> </ul> </li> <li> <p>XMC-GAN: 2021. 1 (CVPR 2021) | Google Research</p> <ul> <li>Text-to-Image</li> </ul> </li> <li> <p>\ud83d\udccc Improved DDPM: 2021. 2 | OpenAI</p> <ul> <li>DDPM\uc758 \uc5ec\ub7ec \uc131\uc9c8\ub4e4\uc744 \uc774\ud574\ud558\ub294\ub370 \uc911\uc694\ud55c \ub17c\ubb38. \uac1c\uc778\uc801\uc73c\ub85c \uc5b4\ub5a4 \uac78 \ud0d0\uad6c\ud558\uae30 \uc704\ud574 \uc774\uac8c \uc65c \uc798\ub418\uc5c8\ub294\uc9c0 \uc2e4\ud5d8\ud574\ubcf4\ub824\uba74 \uc774\ub807\uac8c \ud574\uc57c\ud558\uc9c0 \uc54a\ub098\ub77c\ub294 \uc0dd\uac01.<ul> <li>DDPM\uc774 log-likelihood \uba74\uc5d0\uc11c \ub2e4\ub978 Generative model \ubcf4\ub2e4 \ubc00\ub9ac\ub294 \uc774\uc720<ul> <li>log-likelihood \uc2e4\ud5d8\ud558\ub294 \uc911 conditional-GAN\uacfc \ube44\uad50\ud574\uc57c\ud574\uc11c timestep embedding\uc5d0 class-embedding\uc744 \uac19\uc774 \ub179\uc5ec\ub123\uc5c8\ub294\ub370 \uc774\uac83\uc774 conditional-diffusion\uc758 \uc2dc\uc791\uc774 \uc544\ub2cc\uac00 \ub77c\ub294 \uc0dd\uac01\ub3c4 \ub4e0\ub2e4</li> </ul> </li> <li>Noise scheduling\uc744 linear\uc5d0\uc11c cosine\uc73c\ub85c \ubc14\uafb8\uc790!: linear\ub294 \ud6c4\ubc18 step\uc5d0\uc11c \ub108\ubb34 \uc774\ubbf8\uc9c0\ub791 \uad00\ub828 \uc5c6\ub294 noise-to-noise recon \ubc16\uc5d0 \ud558\uc9c0 \uc54a\ub294\ub2e4.<ul> <li>VLB\ub97c prior / posterior\uc758 interpolate\ub41c \uac83\uc744 L_simple\uc5d0\uc11c \uac19\uc774 \uc4f0\uba74 \uc798\ub098\uc628\ub2e4 (\ud558\uc9c0\ub9cc \uc774\ub610\ud55c \ubcf4\uc815\uc774 \ud544\uc694\ud588\uc74c)</li> <li>\uc774 \uacfc\uc815\uc5d0\uc11c interpolation\uc774 rescaling\uc73c\ub85c \uc778\ud574 shorter diffusion\uc744 \uac00\ub2a5\ud558\uac8c \ud55c\ub2e4... \uc989 \ub354 \uc801\uc740 sampling step\uc744 \uc4f8 \uc218 \uc788\ub2e4\ub294\ub370 \uc815\ud655\ud788 \uc774\ud574\ub294 \uac00\uc9c0 \uc54a\ub294\ub2e4.</li> </ul> </li> <li>Sampling step T\uac00 \ud074\uc218\ub85d \uc798\ub098\uc628\ub2e4(\uc0ac\uc2e4 \uc5b4\ub290\uc815\ub3c4 \uc608\uc0c1\ud588\uc74c). \uadf8\ub7f0\ub370 T\uac00 \ub298\uba74 sampling\uc774 \uc544\uc8fc \ub290\ub9ac\ub2e4 \u2192 \ube68\ub9ac \ubf51\ub294 \ubc95\uc744 \ub5a0\uc62c\ub824\ubcf4\uc558\ub2e4.</li> <li>Model scaling \uad00\uc810\uc5d0\uc11c\ub3c4 \uace0\ubbfc\uc744 \ud588\uc74c: \uc55e\uc73c\ub85c GPU\ub294 \ube75\ube75\ud574\uc9c8 \uac83\uc774\ubbc0\ub85c \ub354 \ud070 \ubaa8\ub378\ub3c4 \ud14c\uc2a4\ud2b8\ud574\ubcf4\uc558\ub2e4...\u314b\u314b;</li> </ul> </li> </ul> </li> <li> <p>DallE: 2021. 2 | OpenAI</p> <ul> <li>Discrete variational autoencoder (DVAE)\ub85c \uc774\ubbf8\uc9c0\ub97c $32^2$ grid\ub85c \ub9cc\ub4e4\uc5b4\uc90c. \uc774 \ub54c codebook\uc740 \ucd1d 8192\uac1c\uc758 \uace0\uc720 token\uc774 \uc788\uc74c. \uc774\ub97c 256 BPE-encoded text token\uc73c\ub85c text query\ub97c toeknize\ud558\uace0 \ubc29\uae08 \ub9cc\ub4e0 $32^2$ \uc774\ubbf8\uc9c0\uc758 image token\ub4e4\uacfc concatenate, autregressive\ud558\uac8c \ud559\uc2b5</li> <li>\uc598\ub3c4 Likelihood $\\ln p_{\\theta,\\psi}(x, y)$\ub97c \ud559\uc2b5\ud558\ub294 \uac83\uc73c\ub85c Lower bound\ub97c \ud559\uc2b5\ud55c\ub2e4.</li> </ul> </li> <li>\ud83d\udccc CLIP: 2021. 3 | OpenAI<ul> <li>Generative modeling\uc774\ub77c\uae30 \ubcf4\ub2e4\ub294 Image-text pair \ud559\uc2b5 \ubc29\ubc95\ub860\uc73c\ub85c \uc774\ub807\uac8c \ud559\uc2b5\ub41c encoder\ub4e4\uc744 \ud65c\uc6a9\ud55c\ub2e4.</li> </ul> </li> <li>SR3: 2021. 4 | Google Research</li> <li> <p>\ud83d\udccc Guided Diffusion (Classifier Guidance, ADM: Ablated Diffusion Model) 2021. 5 | OpenAI</p> <ul> <li>Pre-trained classifier\uc758 gradient\ub97c condition\uc73c\ub85c \ud65c\uc6a9\ud558\ub294 \ubc29\ubc95 \uc81c\uc2dc</li> <li>\uadf8 \ubc16\uc5d0\ub3c4 \ubaa8\ub378 \uad6c\uc870 \uac1c\uc120\uc5d0 \ub300\ud55c \uace0\ucc30 \uc9c4\ud589<ul> <li>Increased depth: \uc131\ub2a5\uc740 \ub192\uc774\uc9c0\ub9cc \uc2dc\uac04\uc774 \ub108\ubb34 \uc624\ub798 \uac78\ub9bc \u2192 discard</li> </ul> </li> <li>\uc0ac\uc2e4 GAN\uc5d0\uc11c Classifier guidance\uc640 \ube44\uc2b7\ud55c \ud6a8\uacfc\ub97c \uc8fc\ub294 \uac83\uc740 truncation, temperature scaling\uc774\ub2e4. \uac19\uc740 \uac78 diffusion\uc5d0\ub3c4 \uc801\uc6a9\ud574\ubcf4\uc558\uc73c\ub098 \uc798\ub418\uc9c0 \uc54a\uc558\ub2e4: blurry/smooth images, low precision &amp; recall<ul> <li>sampling\ud560 \ub54c temperature\ub9cc\ud07c mean\uc740 scaling, deviation\uc740 \ub098\ub220\uc8fc\uae30</li> </ul> </li> </ul> </li> <li> <p>Cascaded Diffusion (CDM) 2021. 6 | Google Research</p> </li> <li>ImageBART: 2021. 8 (NIPS 2021) | CompVis</li> <li>Vit-VQGAN: 2021. 9 | Google Research</li> <li>LSGM: 2021. 10</li> <li>\ud83d\udccc Classifier Free Guidance: 2021 NeurIPS workshop | Google Research<ul> <li>Classifier guidance\ub294 Sampling \uc870\uc808\uc5d0 \uc5c4\uccad\ub09c \uc601\ud5a5\uc744 \ub07c\ucce4\uc73c\ub098 Pre-trained classifier\uc758 gradient\ub97c \uc694\uad6c\ud558\uae30 \ub54c\ubb38\uc5d0 \uad49\uc7a5\ud788 computational burden\uc774 \ub192\ub2e4.</li> <li>\uadf8 \ud558\ub098\uc758 \ubaa8\ub378\uc774 condition\uae4c\uc9c0 \uac19\uc774 \ubc1b\uc544\uc11c guidance\ub97c \uc0dd\uc131\ud558\ub294\ub370, \uc774 \ub54c \uc608\uce21\ud558\ub824\ub294 noise image = conditional + guidance_scale * (conditional - unconditional)<ul> <li>\uc774\ub294 p(c|z_t)\uac00 Bayes rule\uc5d0 \uc758\ud574 p(z_t | c) / p(z_t)\uc5d0 \ube44\ub840\ud558\ub2e4\ub294 \uc810\uc744 log\ub97c \uc50c\uc6cc \uc720\ub3c4\ud55c \uac83\uc73c\ub85c \uc774\ud574\ud558\uba74 \ub41c\ub2e4. \uc989 \uc774 \uc774\ubbf8\uc9c0\ub97c \ud559\uc2b5\uc2dc\ud0a8\ub2e4\ub294 \uac83\uc740 p(c|z_t)\uac00 \uac00\ub2a5\ud558\ub3c4\ub85d \uc0dd\uc131\ud558\ub294\uac74\ub370 t-step\uc758 noise\uac00 \ub080 latent vector\ub85c\ubd80\ud130 class\ub97c \uc608\uce21\ud558\ub294 \uac83\uc744 \ub73b\ud55c\ub2e4.</li> </ul> </li> <li>? \uc804\ubd80\ud130 \uad81\uae08\ud588\ub294\ub370 \uc5ec\uae30\uc11c \uc774 \ub584\uae4c\uc9c0\uc758 guidance\ub294 text prompt\uac00 \uc544\ub2c8\ub77c \uc77c\ubc18\uc801\uc778 ImageNet Class 1000\uac1c\ub97c \uc784\ubca0\ub529\ud574\uc11c \ub123\uc5b4\ub193\ub294 \ud615\uc2dd\uc778\uac00? \uc774\ub807\uae30 \ub54c\ubb38\uc5d0 \ubc14\ub85c \ub2e4\uc74c\uc5d0 \ub098\uc624\ub294 GLIDE\uac00 text-prompt\ub97c CLIP\uc73c\ub85c \ub9cc\ub4e4\uc5c8\ub2e4\ub294 \uac83\uc5d0 \ub300\ud574 \uc694\uc9c0\ub97c \ub450\ub294 \uac83 \uac19\uc74c</li> </ul> </li> </ul> <ul> <li> <p>GLIDE: 2021. 12 | OpenAI</p> <ul> <li>Diffusion + Text Prompt Guidance\uc758 \uc2dc\ucd08\ub77c\uace0 \ubcfc \uc218 \uc788\uc74c (text-to-image synthesis\ub97c \ucc98\uc74c \ud55c \uac8c \uc544\ub2c8\ub77c diffusion\uc5d0)<ul> <li>\uc774 \ub54c text encoder embedding\uc744 CLIP Guidance vs. Classifier-free guidance \ub450 \uac00\uc9c0 \ub2e4\ub978 \ubc29\ubc95\uc744 \uc0ac\uc6a9\ud558\uc5ec \ubaa8\ub378\uc5d0\uac8c \uc81c\uacf5\ud574 \uc774\ubbf8\uc9c0\ub97c \uc0dd\uc131\ud588\ub2e4. \ub450 \uac00\uc9c0 \ubc29\ubc95\uc73c\ub85c \ub098\uc628 \uc774\ubbf8\uc9c0\ub97c \uc0ac\ub78c\uc5d0\uac8c \ubcf4\uc5ec\uc918\uc11c(+automated evaluation) \ub204\uac00 \ub354 \uc9c4\uc9dc \uac19\uc740\uc9c0 \ube44\uad50\ud588\uc744 \ub54c CFG\uac00 \uc774\uacbc\ub2e4. \ucc38\uace0\ub85c \uc5ec\uae30\uc11c CLIP Guidance\ub294 classifier-guidance\uc778\ub370, classifier\uac00 CLIPI text-encoder\uc778 \uacbd\uc6b0\ub97c \ub73b\ud55c\ub2e4.</li> <li>CFG\uac00 photorealistic + wide breadth of world knowledge\ub97c \uac00\uc9c4\ub2e4\uace0 \ud310\ub2e8. DallE\ub791 \ube44\uad50\ud588\uc744 \ub54c GLIDE\uac00 \ub354 photorealistic\ud558\ub2e4\uace0 85%\uac00 \ub2f5\ubcc0, caption\uacfc align\ub418\uc5c8\ub2e4\uace0 69%\uac00 \ub2f5\ubcc0</li> </ul> </li> <li>\uc77c\ubc18\uc801\uc778 Diffusion model\uc744 Text-prompt\ub97c \uac19\uc774 \ub123\uc5b4\uc11c \ud559\uc2b5\ud55c \ubc29\uc2dd</li> </ul> </li> <li> <p>MaskGIT: 2022. 2 | Google Research</p> <ul> <li>Auto-regressive \uae30\ubc18\uc758 \uc774\ubbf8\uc9c0\uc0dd\uc131 \ubaa8\ub378\uc740 \ub108\ubb34 \uc624\ub798 \uac78\ub9b0\ub2e4. \uadf8\ub9ac\uace0 AR \ubc29\uc2dd \uc790\uccb4\uac00 \uc0ac\ub78c\uc774 \uadf8\ub9bc \uadf8\ub9b4 \ub54c\uc758 \uc0c1\ud669\uacfc \ube44\uad50\ud574\ubcf4\uba74 \ub9d0\uc774 \ub418\uc9c0 \uc54a\ub294\ub2e4 (\uc0ac\ub78c\uc740 \uadf8\ub9bc\uadf8\ub9b4\ub54c \uc67c\ucabd \uc704\ubd80\ud130 \uc624\ub978\ucabd \uc544\ub798\ub97c \ucc28\ub840\ub85c \ucc44\uc6b0\ub294 \uac83\uc774 \uc544\ub2c8\ub77c \uc5ec\uae30 \uadf8\ub838\ub2e4\uac00 \uc800\uae30 \uadf8\ub838\ub2e4\uac00 \ud558\uae30 \ub54c\ubb38)</li> <li>\uc774 \ub450 \uac00\uc9c0 motivation\uc73c\ub85c\ubd80\ud130 Parallel decoding\ud55c \ubc29\ubc95\uc744 \uc81c\uc2dc\ud55c\ub2e4. \uae30\uc874 VQGAN\ucc98\ub7fc quantized image token -&gt; recon\ud558\ub294 task\uc640 \ub3d9\uc2dc\uc5d0 MLM\uc758 \ubc29\uc2dd\uacfc \ube44\uc2b7\ud55c MVTM(Masked Visual Token Modeling)\ub97c \uc218\ud589\ud558\uc5ec decoding stage \ub610\ud55c \ud559\uc2b5\ud55c\ub2e4. \uc774\ub294 bidirectional\ud558\uac8c token\uc774 \ub2e4\ub978 \ubaa8\ub4e0 token\ub4e4\uc744 \ucc38\uc870\ud558\uc5ec \uc774\ubbf8\uc9c0\ub97c \ub9cc\ub4e4\uc5b4\ub0bc \uc218 \uc788\ub2e4. (AR \ubc29\uc2dd\uc758 \uacbd\uc6b0 \uc55e\uc5d0\uc11c\ubd80\ud130 \ucc28\uadfc\ucc28\uadfc decode\ud558\ub2e4\ubcf4\ub2c8 \ub4a4\ub97c \ucc38\uc870\ud558\uc9c0\ubabb\ud568). \uc989 \uc774\ubbf8\uc9c0\uac00 \ub3d9\uc2dc\uc5d0 masking\uacfc class token \uc11c\ub85c\ub97c \ucc38\uc870\ud558\uba70 \ud63c\ub3c8\uc758 \uce74\uc624\uc2a4\ub85c\ubd80\ud130 \ub9cc\ub4e4\uc5b4\uc9c0\ub294 \uc774\ubbf8\uc9c0\ub784\uae4c. \uae30\uc874\uc758 32*32=1024 \ud1a0\ud070\ub9f5\uc744 \uc608\uce21\ud558\ub294 task\ub77c\uba74 \ub9d0\uadf8\ub300\ub85c 1,024 step\uc744 \ubc1f\uc544\uc57c \uc774\ubbf8\uc9c0\uc0dd\uc131\uc774 \uac00\ub2a5\ud588\ub2e4\uba74, MaskGIT\uc740 8 ~ 12step \uc548\uc5d0 \uc0dd\uc131\uc744 \ub9c8\uce60 \uc218 \uc788\ub2e4.</li> <li>\uc811\uadfc\ub3c4 \uc2e0\uc120\ud588\uace0 ablation\ub3c4 \uc798\ub41c\ud3b8\uc774\ub77c \uc88b\uc740 \ub17c\ubb38\uc774\ub77c\uace0 \uc0dd\uac01\ud55c\ub2e4.</li> </ul> </li> <li> <p>\ud83d\udccc Latent Diffusion (LDM, or a.k.a. Stable Diffusion) 2022.4 | CompVis</p> <ul> <li>Pixel space\uc5d0\uc11c \ubc14\ub85c \uc608\uce21\ud558\uba74 \uc2dc\uac04\uc774 \ub108\ubb34 \uc624\ub798 \uac78\ub9ac\uae30 \ub54c\ubb38\uc5d0 latent space\uc5d0\uc11c \uc608\uce21\ud560 \uc218 \uc788\ub3c4\ub85d \uc5f0\uad6c. \ud655\uc2e4\ud788 \uc2dc\uac04\uc801\uc778 \uce21\uba74\uc5d0\uc11c \ub2e8\ucd95\uc774 \ub9ce\uc774 \uc774\ub904\uc84c\uc74c.</li> <li>Latent representation\uc5d0 DM\uc744 \uac70\ub294 \uac83\uc5d0 \ub300\ud55c \uace0\ucc30\uc744 \ud588\uc74c<ul> <li>\uae30\ubcf8\uc801\uc73c\ub85c Generative modeling\uc740 perceptual / semantic\ud55c \uc694\uc18c\ub97c \ucc28\ub840\ub85c \ud559\uc2b5\ud558\ub294\ub370, latent representation\ub294 high-frequency detail\uc744 \uc5c6\uc560\uba74\uc11c \uc774\ub97c \ud559\uc2b5(?) \uc774\uac8c \uc815\ud655\ud788 \ubb50\uc9c0</li> </ul> </li> <li>Super resolution, Inpainting task</li> </ul> </li> <li> <p>DallE 2(unCLIP): 2022. 4 | OpenAI</p> <ul> <li>CLIP Objective\ub97c \ud1b5\ud574 \uc8fc\uc5b4\uc9c4 text query &amp; image query\uc5d0 \ub9de\ucdb0\uc11c image / text encoder\ub97c \ud559\uc2b5\ud558\uace0, generation process\uc5d0\uc11c\ub294 CLIP Text embedding\uc744 \ud1b5\ud574 prior\ub97c \ub9cc\ub4e4\uc5b4\uc900 \ud6c4 diffusion decoder\uc5d0 \ub123\uc5b4\uc8fc\ub294 \ud615\ud0dc\ub85c generation\uc744 \uc9c4\ud589\ud55c\ub2e4.</li> <li>CLIP Encoder + Diffusion Decoder \uad6c\uc870<ul> <li>\uc77c\ubc18\uc801\uc73c\ub85c deterministic\ud55c decoder\uc640 \ub2e4\ub974\uac8c non-deterministic\ud558\uac8c \ub3cc\uc544\uac04\ub2e4 (stochastic term\uc774 \uc788\ub294 \uac83 \uac19\ub2e4). \uc774 non-deterministic\ud55c \uc131\uc9c8\uc774 deterministic\ud55c GAN\ub4e4\uc774 \ube44\uc2b7\ud55c \uc774\ubbf8\uc9c0\ub97c \ub0b4\ubc49\ub294 \uac83\uacfc \ub2e4\ub974\uac8c \uad49\uc7a5\ud788 \ub2e4\uc591\ud55c \uc774\ubbf8\uc9c0\ub97c \ub9cc\ub4e4\uc5b4\ub0bc \uc218 \uc788\ub2e4\uace0 \ud55c\ub2e4.</li> </ul> </li> <li>Encoder: Image embedding with text prompt -&gt; \uc784\ubca0\ub529\uc744 \ub0b4\ub193\uc73c\uba74 decoder\ud55c\ud14c text prompt\ub791 \uac19\uc774 \ub118\uaca8\uc11c generate\ud558\ub294 \uad6c\uc870<ul> <li>\uc5ec\uae30\uc11c encoder\uac00 image embedding\uc744 \ud558\ub294 \uacfc\uc815\uc774 \ub450 \uac00\uc9c0\ub85c \ub098\ub25c\ub2e4: AR vs. Diffusion<ul> <li>AR: Text-encoding + Text Embedding + Time Embedding + Image Embedding + Learned Queries\ub97c \uc2dc\uc791\uc810\uc73c\ub85c Auto-regressive\ud558\uac8c z_i\ub97c \ucd94\ub860\ud558\uace0 l2\ub85c \ud559\uc2b5</li> <li>Diffusion: \uc5bb\uc5b4\ub0b8 Image embedding\uc744 LDM \ud615\uc2dd\uc73c\ub85c \ud559\uc2b5 - image embedding\uc744 normal\ub85c \ub9cc\ub4dc\ub294 forward process \uadf8\ub9ac\uace0 \uc6d0\ubcf5\ud558\ub294 \ud615\ud0dc\ub85c \ub9cc\ub4e4\uc5b4\uc8fc\uace0 \uc5ec\uae30\uc5d0 conditioning\uc744 \ud558\uba74\uc11c \uc774\ubbf8\uc9c0\uac00 </li> </ul> </li> </ul> </li> </ul> </li> <li> <p>Imagen: 2022. 5 | Google Research</p> <ul> <li>LM\uc758 \uc5ed\ud560\uc774 \ub9e4\uc6b0 \uc911\uc694\ud588\ub2e4! Frozen Pre-trained LLM\uc73c\ub85c \ud29c\ub2dd\ud55c \uccab \uc0ac\ub840\ub85c \ubd10\uc57c\ud560\ub4ef</li> <li>Classifier guidance\uc758 weight\ub97c dynamic thresholding\uc744 \uc801\uc6a9<ul> <li>Noise prediction\ud55c \uc774\ubbf8\uc9c0\uc758 absolute value\ub85c\ubd80\ud130 percentile\uc744 \uad6c\ud574\uc11c \uadf8 \uac12\uc744 \ud65c\uc6a9: \uc774\uac8c \uc5b4\ub5bb\uac8c \uc720\uc758\ubbf8\ud55c\uc9c0 \uc798\ubaa8\ub974\uaca0\ub2e4.</li> </ul> </li> <li>Efficient U-Net \uad6c\uc870: \uc81c\uc77c \ud070 \uac74 self-attention\uc774 \uc0ac\ub77c\uc9c4 \uac83 \uac19\ub2e4. text\uc640\uc758 cross-attention\uc740 \ub0a8\uaca8\ub450\uc5c8\ub2e4.</li> <li>DrawBench Dataset \uacf5\uac1c</li> </ul> </li> <li> <p>Parti: 2022. 6 | Google Research</p> <ul> <li>Pathways AutoRegresstive Text-to-Image Models\uc758 \uc904\uc784\ub9d0\ub85c seq2seq \ud615\ud0dc\ub85c \ubb38\uc81c\ub97c \ud47c\ub2e4. \ubcf4\ud1b5 NMT\uc5d0\uc11c \ud55c \uc5b8\uc5b4\ub85c\ubd80\ud130 \ub2e4\ub978 \uc5b8\uc5b4\ub85c \ubc88\uc5ed\ud558\ub294 \ud615\uc2dd\uc774\ub77c\uba74, text-prompt\ub97c source sequence, image token\uc744 target sequence\ub85c \ub193\uace0 \ud480\uc5c8\ub2e4\ub294 \uc810.</li> <li>Encoder-decoder style: Decoder-only \ud615\ud0dc (\ubcf4\ud1b5 text-image concat\ud558\uace0 \ud558\ub098\uc529 AR\ub85c \ubf51\uc544\ub0b4\ub294 \uad6c\uc870)\uc5d0\uc11c \ubcc0\ud654\ub97c \uac00\ud588\ub2e4. text\ub294 encode, image\ub294 encoded text\ub97c KV\ub85c \ubc1b\uc544 AR\ud615\ud0dc\ub85c \ubf51\uc544\ub0b4\ub294 \uad6c\uc870 (\ub531 seq2seq)</li> <li>PartiPrompt: 1600\uac1c\uc758 prompt\ub85c \ub2e4\uc591\ud55c \uce74\ud14c\uace0\ub9ac\uc640 \ubaa8\ub378\uc758 \ubb3c\uccb4 \uc774\ud574\ub825\uc744 \ud3c9\uac00\ud560 \uc218 \uc788\ub294 \ud504\ub86c\ud504\ud2b8\uac00 \ud3ec\ud568\ub418\uc5b4 \uc788\ub2e4.</li> </ul> </li> <li>EDM (Denoise Score Matching): 2022. 10 (NeurIPS 2022) | NVIDIA</li> <li>Variational Diffusion Models: 2022. 12 | Google Research</li> <li>MUSE: 2023. 1 | Google Research<ul> <li>VQGAN\uc758 discrete token\uc73c\ub85c \uc870\uae08 \ub354 efficient\ud558\uac8c \ubf51\uc544\ub0bc \uc218 \uc788\ub2e4.</li> <li>Parallel decoding \ub610\ud55c \ud65c\uc6a9\ud588\ub2e4\ub294\ub370 \ucf54\ub4dc\uac00 \uc5c6\uc5b4\uc11c \uc798 \ubaa8\ub974\uaca0\ub2e4. DETR \ub9d0\uace0\ub294 \ub0b4\uac00 \uc544\ub294 \uc0ac\ub840\uac00 \uc5c6\uc5b4\uc11c \uc54c\uae30\uac00 \uc5b4\ub835\ub2e4.</li> </ul> </li> <li> <p>ControlNet: 2023. 2 | Stanford</p> </li> <li> <p>Consistency Models: 2023. 3 | OpenAI</p> </li> </ul>"},{"location":"03_dl/1_diffusions/tldr/#-high-resolution-fidelity-sr","title":"- High-resolution fidelity\ub97c \uc704\ud574 SR\uc744 \ub2e8\uacc4\ubcc4\ub85c \uc9c4\ud589","text":""},{"location":"03_dl/1_diffusions/tldr/#generative-metrics","title":"Generative Metrics","text":""},{"location":"03_dl/1_diffusions/tldr/#inception-score","title":"Inception Score","text":"<ul> <li>Multiplication of Sharpness (S) and Diversity (D), obtained by Inception model. This is somewhat correlated to human evaluation but may not cover the whole thing.</li> </ul> <p>| Inception Score (IS) was proposed by Salimans et al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS [3].</p>"},{"location":"03_dl/1_diffusions/tldr/#frechet-inception-distance-fid","title":"Frechet Inception Distance (FID)","text":"<ul> <li>Caclulates feature space distance with Frechet Distance. FD calculates two normal distributions via sum of mean difference square and std difference square. This does NOT take how synthetic images compare to real images into account.</li> <li>Reference: How to Evaluate GANs using FID</li> </ul> <p>| To better capture diversity than IS, Fr\u00e9chet Inception Distance (FID) was propose judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 [62] latent space. Recently, sFID was proposed by Nash et al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure</p>"},{"location":"03_dl/1_diffusions/tldr/#precision-and-recall-20194-nvidia","title":"Precision and Recall | 2019.4 NVIDIA","text":"<p>| Finally, Kynk\u00e4\u00e4nniemi et al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall).</p>"},{"location":"03_dl/1_diffusions/tldr/#clip-score","title":"CLIP Score","text":""},{"location":"03_dl/1_diffusions/tldr/#cas-classification-accuracy-score","title":"CAS: Classification Accuracy Score","text":"<p>Train ResNet-50 classifier soley on samples generated by the candidate model, and then measuring the classifier's classification accuracy on the ImageNet validation set.</p>"},{"location":"03_dl/1_diffusions/tldr/#generative-loss","title":"Generative Loss","text":"<ul> <li>Perceptual Loss</li> </ul>"},{"location":"03_dl/1_diffusions/tldr/#research-groups","title":"Research Groups","text":"<ul> <li> <p>CompVis Group</p> <ul> <li>Computer Vision and Learning research group at Ludwig Maximilian University of Munich (formerly Computer Vision Group at Heidelberg University)</li> <li>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patric Esser, Bjorn Ommer</li> </ul> </li> <li> <p>Google Research</p> <ul> <li>Jonathan Ho</li> <li>Tim Salimans</li> <li>Christian Saharia</li> <li>Ben Poole</li> <li>Diederiek P. Kingma</li> <li>Jarred Barber</li> <li>Yang Song</li> <li>Abhishek Kumar</li> <li>Jascha Sohl-Dickstein</li> <li>Diederik P. Kingma</li> </ul> </li> <li> <p>OpenAI</p> <ul> <li>Prafulla Dhariwal</li> <li>Radford Alec</li> <li>Jong Wook Kim</li> <li>Chris Hallacy</li> <li>Gabriel Goh</li> <li>Aditya Ramesh</li> <li>Alex Nichol</li> </ul> </li> <li> <p>NVIDIA</p> <ul> <li>Karras Tero</li> <li>Miika Aittala</li> <li>Timo Aila</li> <li>Samuli Laine</li> </ul> </li> <li> <p>Stanford</p> <ul> <li>Jiaming Song</li> <li>Chenlin Meng</li> <li>Stefano Ermon</li> </ul> </li> </ul>"},{"location":"03_dl/2_ssl/1_dino/","title":"DINO: Emerging Properties in Self-Supervised Vision Transformers","text":"<ul> <li>Paper Link</li> <li>GitHub</li> </ul> <p>TL;DR</p> <ul> <li>Aims to impact of SSL on ViT, but not from convnets</li> <li>SSL ViT Features contain explicit information about the semantic segmentation of an image</li> <li>These features are also excellent k-NN classifiers (78.3% top-1 ImageNet)</li> <li>Linear Evaluation 80.1% top-1 ImageNet</li> </ul>"},{"location":"03_dl/2_ssl/1_dino/#introduction","title":"Introduction","text":"<ul> <li>ViT is competitive with convnets, but requires more training data and features do not exhibit unique properties</li> <li>\uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uc11c \uac70\ub454 Transformer\uc758 \uc131\uacf5\uc774 CV\uc5d0\uc11c\ub294 \uc65c \uc77c\uc5b4\ub098\uc9c0 \uc54a\uc744\uae4c? \uc5d0\uc11c \ucc29\uc548\ud55c \ub17c\ubb38<ul> <li>Transformer\ub77c\ub294 \uad6c\uc870 \uc790\uccb4\ub3c4 \uc720\uc758\ubbf8\ud558\uc9c0\ub9cc, self-supervised learning\uc744 transformer architecture \uc704\uc5d0 \ud55c \uac83\uc5d0\uc11c \ud070 \uc131\uacf5\uc744 \uc5bb\uc740 \uac83\uc774 \uc544\ub2cc\uac00\ub77c\ub294 \uc9c8\ubb38\uc5d0\uc11c \uc2dc\uc791</li> </ul> </li> <li>\uadf8\ub3d9\uc548 mode collapse\ub97c \ud53c\ud558\uac70\ub098 \ud558\ub294 \ub4f1\uc758 \ubc29\ubc95\uc73c\ub85c SSL\uc744 </li> </ul>"},{"location":"03_dl/2_ssl/2_swav/","title":"SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments","text":"<ul> <li>Paper Link</li> <li>GitHub</li> </ul> <p>TL;DR</p> <ul> <li>Deep Cluster\uc758 \uc800\uc790\ub85c Self-supervised learning\uc758 proxy task\ub97c cluster \uae30\ubc18\uc73c\ub85c \uc81c\uc548\ud588\ub2e4.<ul> <li>\uc5ec\uae30\uc11c \uc11c\ub85c \ub2e4\ub978 augmented image\uc758 feature vector\ub97c \uc0c1\ub300\uc758 prototype\uacfc \uac00\uae4c\uc6cc\uc9c0\ub3c4\ub85d \ub9de\ucdb0\uc8fc\ub294 loss\ub97c \uc81c\uc548\ud55c\ub2e4.</li> </ul> </li> <li>\ub17c\ubb38\uc5d0\uc11c <code>multi-crop</code>\uc774\ub77c\ub294 data augmentation strategy\ub97c \uc81c\uc2dc\ud558\uc600\ub2e4.</li> </ul>"},{"location":"03_dl/2_ssl/tldr/","title":"Self-supervised Learning TL;DR","text":""},{"location":"03_dl/2_ssl/tldr/#transformer-explainability","title":"Transformer Explainability","text":"Paper TL;DR Notes Source Date DINO v2 \uc694\uc998\uac83\ub4e4\uc740 text-image pair\ub85c SSL\uc744 \ud55c\ub2e4. pixel-level\uc5d0\uc11c \uc5bb\uc744 \uc218 \uc788\ub294 \uac8c \ub9ce\uc740\ub370 \uc774\ub7f0 \uc2dd\uc73c\ub85c \uc791\uc5c5\ud558\uba74 text\uc758 \uc601\ud5a5\uc774 \ub108\ubb34 \ud06c\uace0 paired \ub370\uc774\ud130\uc5d0 \uc758\uc874\ud558\uac8c\ub41c\ub2e4. Meta 2023. 4 <p>| | | | |</p>"},{"location":"03_dl/3_xai/tldr/","title":"Explainable AI TL;DR","text":""},{"location":"03_dl/3_xai/tldr/#transformer-explainability","title":"Transformer Explainability","text":"Paper TL;DR Notes Source Date AttCAT Attention weight\ubfd0 \uc544\ub2c8\ub77c feature value\uc758 magnitude + residual connection\uc774 \uace0\ub824\ud55c \ubc29\ubc95\uc744 \uc81c\uc2dc\ud558\uc600\ub2e4. \ud06c\uac8c novelty\uac00 \uc788\ub2e4\uace0 \uc5ec\uaca8\uc9c0\uc9c0\ub294 \uc54a\ub294\ub2e4 (\uadf8\ub7fc \ub108\uac00\ud574\ubcf4\ub358\uac00). \uc804\uccb4 attention\uc744 \uacf1\ud558\ub294 \ud615\uc2dd\uc778 Rollout\uc774 \uc5b4\uc9c0\uac04\ud558\uba74 0\uc73c\ub85c \uc218\ub834\ud558\ub294 \ud604\uc0c1\uc744 \uc5ec\uae30\uc11c\ub3c4 \ud3ec\ucc29\ud560 \uc218 \uc788\uc5c8\ub2e4. Reference\ub85c \ub2ec\uc544\ub454 \ub17c\ubb38\ub4e4\uc740 Transformer explainability\uc5d0\uc11c \uadf8 \uc5ed\ud560\uc774 \uc911\uc694\ud558\uae30 \ub54c\ubb38\uc5d0 \uaf2d \uc815\ub9ac\ud574\ub193\uc790. Rating: 4658 NeurIPS(2022) 2022"},{"location":"03_dl/3_xai/tldr/#visual-counterfactual-explanations","title":"Visual Counterfactual Explanations","text":"Paper TL;DR Notes Source Date Code DVCEs Classifier Guidance \ubc29\uc2dd\uc5d0\uc11c non-robust classifier\ub97c \uc774\uc6a9\ud558\uc600\ub2e4. \ud0c0\uac9f\uc73c\ub85c \ud558\ub294 \ub77c\ubca8\uc774\ubbf8\uc9c0\uc640 \ud604\uc7ac timestep\uc5d0\uc11c \uc6d0\ubcf8\uc774\ubbf8\uc9c0\ub97c denoise\ud55c \uc774\ubbf8\uc9c0\uc640\uc758 \uac70\ub9ac\ub97c \ucd5c\uc18c\ud654\ud558\ub294 \ubc29\ud5a5\uc73c\ub85c Visual Counterfactual\uc744 \ub9cc\ub4e4\uc5b4\ub0c8\ub2e4. \ub610\ud55c cone-regularization\uc73c\ub85c non-robust classifier\uac00 \uac00\uc9c0\ub294 noisy gradient \ubb38\uc81c\ub97c \uc7a1\uc558\ub2e4. Rating: 5677 NeurIPS(2022) 2022 Link"},{"location":"homeworks/","title":"Works to do","text":""},{"location":"homeworks/#1-grafana","title":"1. grafana","text":""},{"location":"homeworks/grafana/","title":"Grafana","text":"<ul> <li>Basic Tutorials</li> <li>Tutorial from RedHat</li> </ul>"}]}
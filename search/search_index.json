{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Ubuntu-setup","text":"<p>This wiki is about tricks afronted while doing deep learnings on server environments</p>"},{"location":"#1-about-python-tricks","title":"1. About python tricks","text":""},{"location":"#2-about-linux-tricks","title":"2. About linux tricks","text":""},{"location":"#3-about-docker-tricks","title":"3. About docker tricks","text":""},{"location":"#4-about-deep-learning-implementations","title":"4. About deep learning implementations","text":""},{"location":"01_python/0_index/","title":"Python Setups","text":""},{"location":"01_python/0_index/#1-install-anaconda","title":"1. Install Anaconda","text":""},{"location":"01_python/1_conda/","title":"Anaconda installation","text":"<ul> <li>Instructions <pre><code>installsh=Anaconda3-2023.03-Linux-x86_64.sh\nwget https://repo.anaconda.com/archive/${installsh}\nshasum -a 256 ${installsh}\nbash ${installsh}\n</code></pre></li> </ul>"},{"location":"01_python/troubleshoots/0_index/","title":"Python Troubleshoots","text":"<p>Collections of shitty python troubleshoots and tricks</p>"},{"location":"01_python/troubleshoots/pytorch/buffer/","title":"<code>torch.register_buffer</code> vs <code>torch.Parameter</code>","text":"<ul> <li><code>buffer</code><ul> <li>Registered in model but NOT optimized</li> </ul> </li> <li><code>torch.Parameter</code><ul> <li>Being trained.</li> </ul> </li> </ul> <p>Reference</p>"},{"location":"01_python/troubleshoots/pytorch/imgaug/","title":"<code>imgaug</code>","text":"<p>Install <code>imgaug</code> from https://github.com/marcown/imgaug. The main package no longer develops </p>"},{"location":"02_linux/1_zsh/","title":"<code>zsh</code>","text":"<p>Why <code>zsh</code>? - <code>zsh</code> is more capable of auto-completion, boosting productivity for developers.</p>"},{"location":"02_linux/1_zsh/#installing-zsh","title":"Installing <code>zsh</code>","text":"Install zsh<pre><code>sudo apt-get update # (1)\nsudo apt install zsh\n</code></pre> <ol> <li>:man_raising_hand: \uc790\uc8fc\ud560\uc218\ub85d \uc794 \uc5d0\ub7ec\uac00 \uc548\uc0dd\uae40</li> </ol>"},{"location":"02_linux/1_zsh/#zsh-as-default-shell","title":"<code>zsh</code> as default shell","text":"<pre><code>chsh -s $(which zsh)\n</code></pre>"},{"location":"02_linux/1_zsh/#pretty-zsh","title":"Pretty ZSH","text":"<pre><code>sh -c \"$(wget -O- https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)\"\ngit clone https://github.com/sindresorhus/pure.git ~/.zsh/pure\ngit clone https://github.com/zsh-users/zsh-syntax-highlighting.git ~/.zsh/zsh-syntax-highlighting\ngit clone https://github.com/zsh-users/zsh-autosuggestions ~/.zsh/zsh-autosuggestions\n</code></pre> <ul> <li>Append the followings to <code>~/.zshrc</code> file.</li> <li><code>source activate</code> may not work if conda is not installed. After installing anaconda, the command will not provoke an error <pre><code>path+=/opt/conda/bin\nTZ=Asia/Seoul\nLANG=C.UTF-8\nLC_ALL=C.UTF-8\nPYTHONDONTWRITEBYTECODE=1\nPYTHONUNBUFFERED=1\nPYTHONIOENCODING=UTF-8\nPYTHONHTTPSVERIFY=0\nfpath+=(\"$HOME/.zsh/pure\")\\nautoload -U promptinit; promptinit\\nprompt pure\nsource ~/.zsh/zsh-syntax-highlighting/zsh-syntax-highlighting.zsh\nsource ~/.zsh/zsh-autosuggestions/zsh-autosuggestions.zsh\nZSH_AUTOSUGGEST_HIGHLIGHT_STYLE='fg=111'\nsource activate\n</code></pre></li> </ul>"},{"location":"02_linux/2_ssh/","title":"<code>ssh</code> Connection","text":""},{"location":"02_linux/2_ssh/#1-starting-ssh","title":"1. Starting <code>ssh</code>","text":"<pre><code>sudo apt-get update\nsudo apt-get install openssh-server\nsudo systemctl restart sshd\n</code></pre>"},{"location":"02_linux/2_ssh/#2-ssh-without-passwd-use-pub-keys","title":"2. <code>ssh</code> without passwd: use pub keys","text":""},{"location":"02_linux/2_ssh/#1-create-a-new-key","title":"(1) Create a New Key","text":"<pre><code>cd ~/.ssh\nssh-keygen -t rsa -f id_rsa\nls\n&gt;&gt;&gt; id_rsa id_rsa.pub\n</code></pre>"},{"location":"02_linux/2_ssh/#2-send-id_rsapub-to-remote-server","title":"(2) Send <code>id_rsa.pub</code> to remote server","text":"<p>Send public key to remote server <pre><code>scp -P 22 id_rsa.pub (user)@(IP):(ABS_PATH)\n</code></pre></p>"},{"location":"02_linux/2_ssh/#3-edit-keys-at-your-remote-server","title":"(3) Edit keys at your remote server","text":"<pre><code>ssh (user)@(IP) -p 22\nmkdir ~/.ssh # ~/.ssh \ub514\ub809\ud1a0\ub9ac\uac00 \uc5c6\ub294 \uacbd\uc6b0\nchmod 700 ~/.ssh\n\ncat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys\nchmod 600 ~/.ssh/authorized_keys\n</code></pre>"},{"location":"02_linux/2_ssh/#4-upload-new-ip-on-config","title":"(4) Upload new IP on Config","text":"<p>Where is ssh configuration?</p> <ul> <li>In general ssh configuration file <code>config</code> stays in your <code>~/.ssh/</code> directory.</li> <li>Basic setup as follows</li> <li>Put your <code>id_rsa</code> key name in <code>IdentityFile</code></li> </ul> <pre><code>Host (HOSTNAME)\n  HostName (IP)\n  User (USERNAME)\n  Port (PORT-NUMBER)\n  IdentityFile ~/.ssh/id_rsa\n</code></pre>"},{"location":"02_linux/2_ssh/#proxy-jumps","title":"Proxy Jumps","text":"<ul> <li>Reference <pre><code>Host HostA\n  HostName hostA\n  User userA\n\nHost HostB\n  HostName hostB\n  User userB\n  ProxyJump HostA\n</code></pre></li> <li>One can add <code>IdentityFile</code> on final target server <code>HostB</code></li> </ul> <p>Who needs proxy?</p> <ul> <li>Some servers are not accessible even with VPN</li> <li>lab-2080 server is the only available server access thorugh VPN and other servers can be accessed via lab-2080 server.</li> <li>Also, VSCode IDE is available for non-accessible servers with VPN with proxy jump settings.</li> </ul>"},{"location":"02_linux/3_gpu/","title":"Nvidia setups for linux","text":"<ul> <li>Follow the official instructions from nvidia. Below 3 paragraphs are most essential.<ul> <li>2. Pre-install</li> <li>3.10 Ubuntu Package Manager Installation</li> <li>4. Driver installation</li> </ul> </li> <li> <p>This worked for me (Mar 22, 2023), by follwoing the instructions <pre><code>sudo apt install cuda-drivers-525\nlspci | grep -i nvidia\nuname -m &amp;&amp; cat /etc/*release\nsudo apt-get install linux-headers-$(uname -r)\nsudo apt autoremove\nsudo apt-get install linux-headers-$(uname -r)\nwget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64/cuda-keyring_1.0-1_all.deb\nsudo dpkg -i cuda-keyring_1.0-1_all.deb\nsudo apt-get update\nsudo apt-get -y install cuda\nsudo dpkg -i cuda-keyring_1.0-1_all.deb\nsudo apt-key del 7fa2af80\nsudo apt-get update\nsudo apt-get install cuda\nsudo apt-get install nvidia-gds\nsudo reboot\nsudo apt-get install cuda-drivers-525\nsudo reboot\nnvidia-smi\n</code></pre></p> </li> <li> <p>Use <code>nvidia-smi</code> to check gpu utilities</p> </li> </ul>"},{"location":"02_linux/3_gpu/#fixing-updating-nvidia-smi","title":"Fixing / Updating nvidia-smi","text":"<p>Sometimes <code>nvidia-smi</code> command does not work with following error. NVIDIA-SMI Error<pre><code>NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n</code></pre></p> <p>Fix with following command <pre><code>sudo apt install --fix-broken\n</code></pre> Reference</p>"},{"location":"02_linux/4_github/","title":"GitHub (<code>gh</code>)","text":"<p><code>gh</code> is a command to authenticate/login to your account. Basic <code>git</code> command is different from <code>gh</code>.</p>"},{"location":"02_linux/4_github/#install-gh","title":"Install <code>gh</code>","text":"<p>Installing gh. One may need 'sudo su' to make it happen<pre><code>type -p curl &gt;/dev/null || sudo apt install curl -y\ncurl -fsSL https://cli.github.com/packages/githubcli-archive-keyring.gpg | sudo dd of=/usr/share/keyrings/githubcli-archive-keyring.gpg \\\n&amp;&amp; sudo chmod go+r /usr/share/keyrings/githubcli-archive-keyring.gpg \\\n&amp;&amp; echo \"deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/githubcli-archive-keyring.gpg] https://cli.github.com/packages stable main\" | sudo tee /etc/apt/sources.list.d/github-cli.list &gt; /dev/null \\\n&amp;&amp; sudo apt update \\\n&amp;&amp; sudo apt install gh -y\n</code></pre> Upgrade gh<pre><code>sudo apt update\nsudo apt install gh\n</code></pre></p>"},{"location":"02_linux/4_github/#github-login-through-cli","title":"GitHub Login through CLI","text":"<p>Then login through <code>gh</code>. If you're running this on your server, choose \"Paste an authentication token\" on third choice, and paste your developer personal access tokens. <pre><code>gh auth login\n</code></pre></p> <p>What is <code>gpg keys</code>?</p> <p>GitHub no longer allows you to type in passwords on CLI, which means no one can clone/pull/commit anything on cli through passwords. Instead, everybody uses Personal access tokens. Lookup here for more details</p>"},{"location":"02_linux/02_03_commands/1_watch/","title":"watch","text":""},{"location":"02_linux/02_03_commands/1_watch/#watch-to-monitor-something","title":"<code>watch</code>: To monitor something","text":"<p>For gpu monitoring <pre><code>watch -d -n 0.5 nvidia-smi\n</code></pre></p> <ul> <li><code>-d</code>: Show difference</li> <li><code>-n</code>: Time interval</li> </ul>"},{"location":"02_linux/02_03_commands/2_tmux/","title":"tmux","text":""},{"location":"02_linux/02_03_commands/2_tmux/#tmux-running-in-background-but-accessible","title":"<code>tmux</code>: Running in background, but accessible","text":"Installing tmux<pre><code>sudo apt-get install tmux\n</code></pre>"},{"location":"02_linux/02_03_commands/2_tmux/#basic-usage","title":"Basic usage","text":"<ul> <li><code>tmux</code> consists of session - pane hierarchy. <code>work</code> is the name of session.</li> <li>New session: <code>tmux new -s work</code></li> <li>Exit session: CTRL B, D</li> <li>Resume to session: <code>tmux attach -t work</code></li> <li>Divide session into pane:<ul> <li>Horizontal spilt: CTRL B, \"</li> <li>Vertical split: CTRL B, %</li> </ul> </li> <li>Entering Command mode: CTRL B, :<ul> <li>Removing panes: <code>kill-pane -t (index)</code>, where index counts from 0</li> </ul> </li> </ul>"},{"location":"02_linux/02_03_commands/3_rsync/","title":"rsync","text":""},{"location":"02_linux/02_03_commands/3_rsync/#rsync-send-files-to-remote-server","title":"<code>rsync</code>: Send files to remote server","text":"<p>Basic rsync usage<pre><code>rsync -e 'ssh -p 20000' LOCALDIR REMOTEDIR\n</code></pre> Flags</p> <ul> <li><code>-v</code>: verbosity</li> <li><code>-r</code>: recursive</li> <li><code>-a</code>: archive mode</li> <li><code>-z</code>: Data compress</li> <li><code>-h</code>: Human-readable</li> <li><code>-e</code>: Extra ssh flags to feed</li> </ul> <p>Sometimes permission issue may be raised. Use <code>chmod -R 777 (TARGET_DIR)</code> to add permission</p> <p>Reference</p> <ul> <li>Rsync basics</li> <li>Changing ports</li> </ul>"},{"location":"02_linux/02_03_commands/4_storage/","title":"storage","text":""},{"location":"02_linux/02_03_commands/4_storage/#checking-storage-for-certain-directory","title":"Checking storage for certain directory","text":"<pre><code>ls | xargs du -sh\n</code></pre> <p>What is <code>xargs</code>?</p> <p>Sometimes you want outputs from a certain command such as list, and feed them to other commands. This is available through <code>xargs</code>. Above command allows <code>ls</code> stdout to be stdin of <code>du -sh</code> command.</p>"},{"location":"02_linux/02_03_commands/5_mount/","title":"mount","text":""},{"location":"02_linux/02_03_commands/5_mount/#mounting-external-disk","title":"Mounting External disk","text":"Mounting external disk<pre><code>sudo fdisk -l\nsudo mkfs.ext4 /dev/sda # (1)\nsudo mount /dev/sda /mnt\n</code></pre> <ol> <li>Please check your disk name: <code>dev/sda</code></li> </ol>"},{"location":"02_linux/02_03_commands/6_files/","title":"Files","text":""},{"location":"02_linux/02_03_commands/6_files/#count-number-of-files","title":"Count number of files","text":"<pre><code>ls | wc -l\n</code></pre> <p>What is <code>|</code>?</p> <p><code>|</code> works as a pipe. This operand will take outputs from previous command and send it to next command. From above example, <code>wc -l</code> will count the number of outputs from <code>ls</code>.</p>"},{"location":"02_linux/02_03_commands/6_files/#removing-certain-files","title":"Removing certain files","text":"For certain files not directories<pre><code>find . -type f -name '*.o'\nfind . -type dir -name '*.o'\n# Remove certain files\nfind . -type f -name '*.o' | xargs rm -r\nfind . -type f -name '*.o' -delete\n</code></pre> <p>For some reasons, this does not delete Reference</p>"},{"location":"02_linux/02_03_commands/7_users/","title":"Ubuntu Users","text":""},{"location":"02_linux/02_03_commands/7_users/#add-user","title":"Add User","text":"Add user<pre><code>adduser 1pha\nsudo usermod -aG sudo 1pha\n</code></pre>"},{"location":"02_linux/02_03_commands/7_users/#checking-out-to-other-user","title":"Checking out to other User","text":"checkout<pre><code>su 1pha\n</code></pre>"},{"location":"02_linux/02_03_commands/8_zip/","title":"Compress &amp; Unzip Files","text":""},{"location":"02_linux/02_03_commands/8_zip/#zip","title":"<code>zip</code>","text":"<p>Note that it's a good convention to <code>.zip</code> extension in <code>(FILENAME)</code> Compress with zip<pre><code>zip (FILENAME) (TARGET_DIR)\n</code></pre></p> <p>If <code>(TARGET_DIR)</code> contains subdirectory, than recursive flag of <code>-r</code> is required. Compress with zip with subdirectories<pre><code>zip -r (FILENAME) (TARGET_DIR)\n</code></pre></p> Unzip<pre><code>unzip (FILENAME)\nunzip (FILENAME) -d (TARGET_DIR)\n</code></pre>"},{"location":"02_linux/02_03_commands/8_zip/#tar","title":"<code>tar</code>","text":"<p>Flags * <code>-z</code>: Compress with gzip(gz) * <code>-j</code>: Compress with bzip2(bz) * <code>-c</code>: collect to tar * <code>-x</code>: extract tar * <code>-f</code>: (Mandatory) Collects file * <code>-v</code>: Verbosity * <code>-p</code>: Compress with ownership</p> <p>What is the difference between <code>.tar</code> and <code>.tar.gz</code>?</p> <p><code>.tar</code> does NOT compress directories but rather collects all files to archive. Compressed <code>.tar</code> file becomes <code>.tar.gz</code>. This uses gunzip to compress files.</p> <p><code>(FILENAME)</code> contains <code>.tar</code> Compress and decompress to tar<pre><code># Compress\ntar -cvf (FILENAME) (TARGETDIR)\n# Decompress\ntar -xvf (FILENAME)\n</code></pre></p> Compress and decompress to tar.gz<pre><code># Compress\ntar -zcvf (FILENAME) (TARGETDIR)\n# Decompress\ntar -zxvf (FILENAME)\n</code></pre>"},{"location":"03_dl/","title":"Deep Learning","text":"<p>Deep learning model implementation will be posted here.</p>"},{"location":"03_dl/0_basics/1_attention/","title":"Basic Attention Codes","text":"Attention<pre><code>class Attention(nn.Module):\ndef __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\nsuper().__init__()\nself.num_heads = num_heads\nhead_dim = dim // num_heads\nself.scale = qk_scale or head_dim ** -0.5\n# \ud55c \ubc88\uc5d0 \uc785\ub825\uc744 q, k, v\ub85c \ub123\uace0 \ucabc\uac2c\nself.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\nself.attn_drop = nn.Dropout(attn_drop)\nself.proj = nn.Linear(dim, dim)\nself.proj_drop = nn.Dropout(proj_drop)\ndef forward(self, x):\nB, N, C = x.shape\n# \uc774\ub7f0 \ubcf5\uc7a1\ud55c \ubc29\uc2dd\uc744 \ud53c\ud558\uae30 \uc704\ud574 einops\ub97c \uc0ac\uc6a9\ud558\uae30\ub3c4\ud568\nqkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\nq, k, v = qkv[0], qkv[1], qkv[2]\nattn = (q @ k.transpose(-2, -1)) * self.scale\nattn = attn.softmax(dim=-1)\nattn = self.attn_drop(attn)\nx = (attn @ v).transpose(1, 2).reshape(B, N, C)\nx = self.proj(x)\nx = self.proj_drop(x)\nreturn x, attn\n</code></pre>"},{"location":"03_dl/0_basics/2_2_swin/","title":"Swin","text":""},{"location":"03_dl/0_basics/2_2_swin/#swin_v2","title":"Swin_v2","text":"<p>TL;DR</p> <ul> <li>Large-scale ViT in computer vision<ul> <li>Residual-post-norm + cosine attention</li> <li>Log-spaced continuous position bias: \uc791\uc740 \uc774\ubbf8\uc9c0 pre-train -&gt; \ud070 \uc774\ubbf8\uc9c0\uc5d0 fine-tune\ud560 \ub54c \uc0dd\uae38 \uc218 \uc788\ub294 relative positional bias\uc758 \uac12 \ud06c\uae30 \ucc28\uc774\ub85c \uc778\ud55c \uc131\ub2a5 \uc800\ud558\ub97c \ub9c9\uae30 \uc704\ud574 positional bias\uc5d0 log\ub9cc \ucd94\uac00\ud588\uc74c</li> <li>\ud559\uc2b5\ud560 \ub54c SSL \ubc29\ubc95\uc744 \uc774\uc6a9\ud588\uc74c</li> </ul> </li> <li>\uc5ec\ub7ec \uc2e4\ud5d8\uc744 \ud1b5\ud574 \uc704\uc5d0 \ub300\ud55c \uadfc\uac70\ub4e4\uc744 \uc815\ub9ac</li> </ul>"},{"location":"03_dl/0_basics/2_2_swin/#introduction","title":"Introduction","text":"<ol> <li> <p>Instability Issue</p> <ul> <li>\ubaa8\ub378\uc758 \uaddc\ubaa8\uac00 \ucee4\uc9c8\uc218\ub85d \ud6c4\ubc18 \ub808\uc774\uc5b4\uc758 activation\uac12\uc774 \ub108\ubb34 \ucee4\uc9c0\uac8c \ub41c\ub2e4 (Figure 2)</li> <li></li> <li>Suggestion<ul> <li>Residual post-normalization</li> <li>Scaled Cosine Attention</li> </ul> </li> </ul> </li> <li> <p>Image Resolution Issue</p> <ul> <li>Pre-trained \ubaa8\ub378\uc774 \ud559\uc2b5\ud588\ub358 resolution\uacfc \ub2e4\ub978 resolution\uc744 \ub123\uc5b4\uc11c \ud559\uc2b5\ud558\uba74 \uc131\ub2a5\uc774 \ud06c\uac8c \uc800\ud558\ub418\ub294 \uc810</li> <li>\uc774\ub294 Relative positional embedding\uc774 \ub108\ubb34 \uac12\uc774 \ud06c\uac8c \ub4e4\uc5b4\uac00\uae30 \ub54c\ubb38\uc5d0 resolution\uc774 \uc624\ub974\uba74 \uc774 embedding\uac12\uc774 vector embedding\uc5d0 \ub9ce\uc740 \uc601\ud5a5\uc774 \uc788\uae30 \ub54c\ubb38\uc73c\ub85c \ud310\ub2e8</li> <li>Suggestion<ul> <li>Log-spaced continuous position bias</li> </ul> </li> </ul> </li> </ol>"},{"location":"03_dl/0_basics/2_vit/","title":"Basic Vision Transformer Block","text":"Patch Embedding<pre><code>class PatchEmbed(nn.Module):\n\"\"\" Image to Patch Embedding\n    \"\"\"\ndef __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):\nsuper().__init__()\nnum_patches = (img_size // patch_size) * (img_size // patch_size)\nself.img_size = img_size\nself.patch_size = patch_size\nself.num_patches = num_patches\nself.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\ndef forward(self, x):\nB, C, H, W = x.shape\nx = self.proj(x).flatten(2).transpose(1, 2)\nreturn x\n</code></pre>"},{"location":"03_dl/1_diffusions/brief_history/","title":"Brief History of Diffusion-based Generative models and their variants","text":"<ul> <li>BigGAN: 2019. 2 (ICLR 2019) | DeepMind</li> <li>Generative Modeling by Estimating Gradients of the Data Distribution: 2019. 7 | Stanford</li> <li>\ud83d\udccc DDPM: 2020. 12 | Google Research</li> <li>DDIM: 2020. 10 | Stanford<ul> <li>\uae30\uc874 DDPM\uc774 Noising process\uac00 Markovian\uc774\ub77c \ubc14\ub85c \uc774\uc804\uc758 timestep noised image\uc5d0\ub9cc \uc758\uc874\ud558\uc5ec \uc804\uccb4 \ud504\ub85c\uc138\uc2a4\ub97c \ub9cc\ub4e4\uc5b4\ub0b4\ub294\ub370 \uc2dc\uac04\uc774 \ub9ce\uc774 \uac78\ub9b0\ub2e4. \uc5ec\uae30\uc11c Non-markovian\uc73c\ub85c $x_0$\uc5d0\uc11c $x_T$\ub85c \uac00\ub294 \uacfc\uc815\uc744 deterministic\ud558\uac8c forward process\ub97c \uc77c\ubd80\ub9cc sampling\ud558\uc5ec train\ud558\uace0 \ucd94\ub860\ud560 \ub54c\ub3c4 \ub9c8\ucc2c\uac00\uc9c0\ub85c \uc804\uccb4 step\uc744 \ub2e4 \uac70\uce58\uc9c0 \uc54a\uc544 accelerated inference\uac00 \uac00\ub2a5\ud558\ub2e4.</li> </ul> </li> <li>Score-based Generative Modeling through Stochastic Differential Equations: 2020. 11 | Google Research</li> <li> <p>VQGAN: 2020. 12 (CVPR 2021) | CompVis</p> <ul> <li>ConvNet\uacfc Transformer\uac00 generation task\uc5d0\uc11c \uac01\uac01 \uc798\ud558\ub294 \ubd80\ubd84\uc744 \ub2e4 \uc0ac\uc6a9\ud574\uc11c image tokenization\uc744 \uc9c4\ud589<ul> <li>ConvNet\uc740 inductive bias\ub85c \uc778\ud574 local \uc815\ubcf4\ub97c \ub9ce\uc774 \uac00\uc9c0\uace0 \uc788\ub2e4. \ud558\uc9c0\ub9cc global \uc815\ubcf4\ub97c \uac00\uc9c0\uace0 \uc788\uc9c0 \uc54a\ub2e4.</li> <li>Transformer model\uc740 low inductive bias \ub355\ubd84\uc5d0 expressive\ud558\ub2e4 (\uc6cc\ub529 \ub54c\ubb38\uc5d0 \ud63c\ub780\uc2a4\ub7ec\uc6b4\ub370 \uc544\ub9c8 global information\uc744 \ubcf4\uc720\ud55c\ub2e4 \uc815\ub3c4\ub85c \uc774\ud574\ud558\uba74 \ub418\uc9c0 \uc54a\uc744\uae4c \uc2f6\ub2e4). \ud558\uc9c0\ub9cc \uace0\ud654\uc9c8\uc758 \uc774\ubbf8\uc9c0\ub97c \ub9cc\ub4e4\uae30\ub294 \uc5b4\ub835\ub2e4.</li> </ul> </li> <li>ConvNet\uc73c\ub85c image feature map\uc744 \ub9cc\ub4e0 \ud6c4 \uc774\ub97c quantize\ud558\uc5ec latent image code\ub97c \ub9cc\ub4e0\ub2e4. \ub610\ud55c \uc774 code\ub97c decoder\uc5d0 \ud0dc\uc6cc GAN \uad6c\uc870\ub85c \ud559\uc2b5\uc744 \uc2dc\ud0a8\ub2e4.</li> <li>\uadf8 \ud6c4 image\ub97c tokenize\ud558\uc5ec sequence\ub85c \ubcf4\uc720\ud55c \ud6c4 \uc774\ub97c language model\ucc98\ub7fc \ud559\uc2b5\uc2dc\ud0a8\ub2e4 (Auto-regressive\ud558\uac8c next token prediction \uac00\uae4c\uc6c0)</li> <li>Auto-regressive\ud55c \ud2b9\uc131\uc744 \ud65c\uc6a9\ud558\uc5ec conditioning\uc744 \ud560 \uc218 \uc788\ub2e4: \uc2dc\uc791\uc810\uc5d0 conditioning information\uc744 prepend\ud574\uc11c \ucd94\ub860</li> </ul> </li> <li> <p>Improved DDPM: 2021. 2 | OpenAI</p> </li> <li>DallE: 2021. 2 | OpenAI<ul> <li>Discrete variational autoencoder (DVAE)\ub85c \uc774\ubbf8\uc9c0\ub97c $32^2$ grid\ub85c \ub9cc\ub4e4\uc5b4\uc90c. \uc774 \ub54c codebook\uc740 \ucd1d 8192\uac1c\uc758 \uace0\uc720 token\uc774 \uc788\uc74c. \uc774\ub97c 256 BPE-encoded text token\uc73c\ub85c text query\ub97c toeknize\ud558\uace0 \ubc29\uae08 \ub9cc\ub4e0 $32^2$ \uc774\ubbf8\uc9c0\uc758 image token\ub4e4\uacfc concatenate, autregressive\ud558\uac8c \ud559\uc2b5</li> <li>\uc598\ub3c4 Likelihood $\\ln p_{\\theta,\\psi}(x, y)$\ub97c \ud559\uc2b5\ud558\ub294 \uac83\uc73c\ub85c Lower bound\ub97c \ud559\uc2b5\ud55c\ub2e4.</li> </ul> </li> <li>\ud83d\udccc CLIP: 2021. 3 | OpenAI<ul> <li>Generative modeling\uc774\ub77c\uae30 \ubcf4\ub2e4\ub294 Image-text pair \ud559\uc2b5 \ubc29\ubc95\ub860\uc73c\ub85c \uc774\ub807\uac8c \ud559\uc2b5\ub41c encoder\ub4e4\uc744 \ud65c\uc6a9\ud55c\ub2e4.</li> </ul> </li> <li>SR3: 2021. 4 | Google Research</li> <li> <p>\ud83d\udccc Guided Diffusion (Classifier Guidance, ADM: Ablated Diffusion Model) 2021. 5 | OpenAI</p> <ul> <li>Pre-trained classifier\uc758 gradient\ub97c condition\uc73c\ub85c \ud65c\uc6a9\ud558\ub294 \ubc29\ubc95 \uc81c\uc2dc</li> <li>\uadf8 \ubc16\uc5d0\ub3c4 \ubaa8\ub378 \uad6c\uc870 \uac1c\uc120\uc5d0 \ub300\ud55c \uace0\ucc30 \uc9c4\ud589<ul> <li>Increased depth: \uc131\ub2a5\uc740 \ub192\uc774\uc9c0\ub9cc \uc2dc\uac04\uc774 \ub108\ubb34 \uc624\ub798 \uac78\ub9bc \u2192 discard</li> </ul> </li> <li>\uc0ac\uc2e4 GAN\uc5d0\uc11c Classifier guidance\uc640 \ube44\uc2b7\ud55c \ud6a8\uacfc\ub97c \uc8fc\ub294 \uac83\uc740 truncation, temperature scaling\uc774\ub2e4. \uac19\uc740 \uac78 diffusion\uc5d0\ub3c4 \uc801\uc6a9\ud574\ubcf4\uc558\uc73c\ub098 \uc798\ub418\uc9c0 \uc54a\uc558\ub2e4: blurry/smooth images, low precision &amp; recall<ul> <li>sampling\ud560 \ub54c temperature\ub9cc\ud07c mean\uc740 scaling, deviation\uc740 \ub098\ub220\uc8fc\uae30</li> </ul> </li> </ul> </li> <li> <p>Cascaded Diffusion 2021. 6 | Google Research</p> </li> <li>ImageBART: 2021. 8 (NIPS 2021) | CompVis</li> <li>LSGM: 2021. 10</li> <li> <p>GLIDE: 2021. 12 | OpenAI</p> </li> <li> <p>\ud83d\udccc Classifier Free Guidance: 2021 NeurIPS workshop | Google Research</p> </li> <li> <p>DallE 2(unCLIP): 2022. 4 | OpenAI</p> <ul> <li>CLIP Objective\ub97c \ud1b5\ud574 \uc8fc\uc5b4\uc9c4 text query &amp; image query\uc5d0 \ub9de\ucdb0\uc11c image / text encoder\ub97c \ud559\uc2b5\ud558\uace0, generation process\uc5d0\uc11c\ub294 CLIP Text embedding\uc744 \ud1b5\ud574 prior\ub97c \ub9cc\ub4e4\uc5b4\uc900 \ud6c4 diffusion decoder\uc5d0 \ub123\uc5b4\uc8fc\ub294 \ud615\ud0dc\ub85c generation\uc744 \uc9c4\ud589\ud55c\ub2e4.</li> </ul> </li> <li> <p>\ud83d\udccc Latent Diffusion (LDM, or a.k.a. Stable Diffusion) 2022.4 | CompVis</p> <ul> <li>Pixel space\uc5d0\uc11c \ubc14\ub85c \uc608\uce21\ud558\uba74 \uc2dc\uac04\uc774 \ub108\ubb34 \uc624\ub798 \uac78\ub9ac\uae30 \ub54c\ubb38\uc5d0 latent space\uc5d0\uc11c \uc608\uce21\ud560 \uc218 \uc788\ub3c4\ub85d \uc5f0\uad6c. \ud655\uc2e4\ud788 \uc2dc\uac04\uc801\uc778 \uce21\uba74\uc5d0\uc11c \ub2e8\ucd95\uc774 \ub9ce\uc774 \uc774\ub904\uc84c\uc74c.</li> <li>Latent representation\uc5d0 DM\uc744 \uac70\ub294 \uac83\uc5d0 \ub300\ud55c \uace0\ucc30\uc744 \ud588\uc74c<ul> <li>\uae30\ubcf8\uc801\uc73c\ub85c Generative modeling\uc740 perceptual / semantic\ud55c \uc694\uc18c\ub97c \ucc28\ub840\ub85c \ud559\uc2b5\ud558\ub294\ub370, latent representation\ub294 high-frequency detail\uc744 \uc5c6\uc560\uba74\uc11c \uc774\ub97c \ud559\uc2b5(?) \uc774\uac8c \uc815\ud655\ud788 \ubb50\uc9c0</li> </ul> </li> <li>Super resolution, Inpainting task</li> </ul> </li> <li> <p>Imagen: 2022. 5 | Google Research</p> <ul> <li>LM\uc758 \uc5ed\ud560\uc774 \ub9e4\uc6b0 \uc911\uc694\ud588\ub2e4! Frozen Pre-trained LLM\uc73c\ub85c \ud29c\ub2dd\ud55c \uccab \uc0ac\ub840\ub85c \ubd10\uc57c\ud560\ub4ef</li> <li>Classifier guidance\uc758 weight\ub97c dynamic thresholding\uc744 \uc801\uc6a9<ul> <li>Noise prediction\ud55c \uc774\ubbf8\uc9c0\uc758 absolute value\ub85c\ubd80\ud130 percentile\uc744 \uad6c\ud574\uc11c \uadf8 \uac12\uc744 \ud65c\uc6a9: \uc774\uac8c \uc5b4\ub5bb\uac8c \uc720\uc758\ubbf8\ud55c\uc9c0 \uc798\ubaa8\ub974\uaca0\ub2e4.</li> </ul> </li> <li>Efficient U-Net \uad6c\uc870: \uc81c\uc77c \ud070 \uac74 self-attention\uc774 \uc0ac\ub77c\uc9c4 \uac83 \uac19\ub2e4.</li> <li>DrawBench Dataset \uacf5\uac1c</li> </ul> </li> <li> <p>Parti: 2022. 6 | Google Research</p> </li> <li>Elucidating the Design space of Diffusions: 2022. 10 | NVIDIA</li> <li>Variational Diffusion Models: 2022. 12 | Google Research</li> <li> <p>MUSE: 2023. 1 | Google Research</p> <ul> <li>VQGAN\uc758 discrete token\uc73c\ub85c \uc870\uae08 \ub354 efficient\ud558\uac8c \ubf51\uc544\ub0bc \uc218 \uc788\ub2e4.</li> <li>Parallel decoding \ub610\ud55c \ud65c\uc6a9\ud588\ub2e4\ub294\ub370 \ucf54\ub4dc\uac00 \uc5c6\uc5b4\uc11c \uc798 \ubaa8\ub974\uaca0\ub2e4. DETR \ub9d0\uace0\ub294 \ub0b4\uac00 \uc544\ub294 \uc0ac\ub840\uac00 \uc5c6\uc5b4\uc11c \uc54c\uae30\uac00 \uc5b4\ub835\ub2e4.</li> </ul> </li> <li> <p>Consistency Models: 2023. 3 | OpenAI</p> </li> </ul>"},{"location":"03_dl/1_diffusions/brief_history/#generative-metrics-loss","title":"Generative Metrics &amp; Loss","text":""},{"location":"03_dl/1_diffusions/brief_history/#metrics","title":"Metrics","text":"<p>| Inception Score (IS) was proposed by Salimans et al. [54], and it measures how well a model captures the full ImageNet class distribution while still producing individual samples that are convincing examples of a single class. One drawback of this metric is that it does not reward covering the whole distribution or capturing diversity within a class, and models which memorize a small subset of the full dataset will still have high IS [3].</p> <p>| To better capture diversity than IS, Fr\u00e9chet Inception Distance (FID) was propose judgement than Inception Score. FID provides a symmetric measure of the distance between two image distributions in the Inception-V3 [62] latent space. Recently, sFID was proposed by Nash et al. [42] as a version of FID that uses spatial features rather than the standard pooled features. They find that this metric better captures spatial relationships, rewarding image distributions with coherent high-level structure</p> <p>| Finally, Kynk\u00e4\u00e4nniemi et al. [32] proposed Improved Precision and Recall metrics to separately measure sample fidelity as the fraction of model samples which fall into the data manifold (precision), and diversity as the fraction of data samples which fall into the sample manifold (recall). * Inception Score (IS) * Precision and Recall Metric | 2019. 4 | NVIDIA * Frechet Inception Distance (FID) * CLIP Score</p>"},{"location":"03_dl/1_diffusions/brief_history/#loss","title":"Loss","text":"<ul> <li>Perceptual Loss</li> </ul>"},{"location":"03_dl/1_diffusions/brief_history/#research-groups","title":"Research Groups","text":"<ul> <li> <p>CompVis Group</p> <ul> <li>Computer Vision and Learning research group at Ludwig Maximilian University of Munich (formerly Computer Vision Group at Heidelberg University)</li> <li>Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patric Esser, Bjorn Ommer</li> </ul> </li> <li> <p>Google Research</p> <ul> <li>Jonathan Ho</li> <li>Tim Salimans</li> <li>Christian Saharia</li> <li>Ben Poole</li> <li>Diederiek P. Kingma</li> <li>Jarred Barber</li> <li>Yang Song</li> <li>Abhishek Kumar</li> <li>Jascha Sohl-Dickstein</li> <li>Diederik P. Kingma</li> </ul> </li> <li> <p>OpenAI</p> <ul> <li>Prafulla Dhariwal</li> <li>Radford Alec</li> <li>Jong Wook Kim</li> <li>Chris Hallacy</li> <li>Gabriel Goh</li> <li>Aditya Ramesh</li> <li>Alex Nichol</li> </ul> </li> <li> <p>NVIDIA</p> <ul> <li>Karras Tero</li> <li>Miika Aittala</li> <li>Timo Aila</li> <li>Samuli Laine</li> </ul> </li> <li> <p>Stanford</p> <ul> <li>Jiaming Song</li> <li>Chenlin Meng</li> <li>Stefano Ermon</li> </ul> </li> </ul>"},{"location":"03_dl/2_ssl/1_dino/","title":"DINO: Emerging Properties in Self-Supervised Vision Transformers","text":"<ul> <li>Paper Link</li> <li>GitHub</li> </ul> <p>TL;DR</p> <ul> <li>Aims to impact of SSL on ViT, but not from convnets</li> <li>SSL ViT Features contain explicit information about the semantic segmentation of an image</li> <li>These features are also excellent k-NN classifiers (78.3% top-1 ImageNet)</li> <li>Linear Evaluation 80.1% top-1 ImageNet</li> </ul>"},{"location":"03_dl/2_ssl/1_dino/#introduction","title":"Introduction","text":"<ul> <li>ViT is competitive with convnets, but requires more training data and features do not exhibit unique properties</li> <li>\uc5b8\uc5b4 \ubaa8\ub378\uc5d0\uc11c \uac70\ub454 Transformer\uc758 \uc131\uacf5\uc774 CV\uc5d0\uc11c\ub294 \uc65c \uc77c\uc5b4\ub098\uc9c0 \uc54a\uc744\uae4c? \uc5d0\uc11c \ucc29\uc548\ud55c \ub17c\ubb38<ul> <li>Transformer\ub77c\ub294 \uad6c\uc870 \uc790\uccb4\ub3c4 \uc720\uc758\ubbf8\ud558\uc9c0\ub9cc, self-supervised learning\uc744 transformer architecture \uc704\uc5d0 \ud55c \uac83\uc5d0\uc11c \ud070 \uc131\uacf5\uc744 \uc5bb\uc740 \uac83\uc774 \uc544\ub2cc\uac00\ub77c\ub294 \uc9c8\ubb38\uc5d0\uc11c \uc2dc\uc791</li> </ul> </li> <li>\uadf8\ub3d9\uc548 mode collapse\ub97c \ud53c\ud558\uac70\ub098 \ud558\ub294 \ub4f1\uc758 \ubc29\ubc95\uc73c\ub85c SSL\uc744 </li> </ul>"},{"location":"03_dl/2_ssl/2_swav/","title":"SwAV: Unsupervised Learning of Visual Features by Contrasting Cluster Assignments","text":"<ul> <li>Paper Link</li> <li>GitHub</li> </ul> <p>TL;DR</p> <ul> <li>Deep Cluster\uc758 \uc800\uc790\ub85c Self-supervised learning\uc758 proxy task\ub97c cluster \uae30\ubc18\uc73c\ub85c \uc81c\uc548\ud588\ub2e4.<ul> <li>\uc5ec\uae30\uc11c \uc11c\ub85c \ub2e4\ub978 augmented image\uc758 feature vector\ub97c \uc0c1\ub300\uc758 prototype\uacfc \uac00\uae4c\uc6cc\uc9c0\ub3c4\ub85d \ub9de\ucdb0\uc8fc\ub294 loss\ub97c \uc81c\uc548\ud55c\ub2e4.</li> </ul> </li> <li>\ub17c\ubb38\uc5d0\uc11c <code>multi-crop</code>\uc774\ub77c\ub294 data augmentation strategy\ub97c \uc81c\uc2dc\ud558\uc600\ub2e4.</li> </ul>"},{"location":"homeworks/","title":"Works to do","text":""},{"location":"homeworks/#1-grafana","title":"1. grafana","text":""},{"location":"homeworks/grafana/","title":"Grafana","text":"<ul> <li>Basic Tutorials</li> <li>Tutorial from RedHat</li> </ul>"}]}